{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5: Understanding VSA Models - Comparative Analysis\n",
    "\n",
    "VSAX provides three VSA models: **FHRR** (complex vectors), **MAP** (real vectors), and **Binary** (discrete vectors). But when should you use each one?\n",
    "\n",
    "This tutorial compares all three models across multiple dimensions to help you make informed decisions.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Compare FHRR, MAP, and Binary on classification tasks\n",
    "- Understand noise tolerance differences\n",
    "- Analyze capacity (how many items can be bundled before interference)\n",
    "- Benchmark speed and memory usage\n",
    "- Learn when to use each model\n",
    "\n",
    "## The Three Models\n",
    "\n",
    "| Model | Representation | Binding | Unbinding | Best For |\n",
    "|-------|----------------|---------|-----------|----------|\n",
    "| **FHRR** | Complex (phase) | Circular convolution (FFT) | **Exact** | Semantic similarity, analogies |\n",
    "| **MAP** | Real-valued | Element-wise multiply | **Approximate** | Speed, interpretability |\n",
    "| **Binary** | Discrete {-1,+1} | XOR (multiply) | **Exact** | Memory efficiency, hardware |\n",
    "\n",
    "Let's put them to the test!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from vsax import create_fhrr_model, create_map_model, create_binary_model\n",
    "from vsax import VSAMemory\n",
    "from vsax.similarity import cosine_similarity\n",
    "from vsax.utils import vmap_similarity\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create All Three Models\n",
    "\n",
    "We'll use the same dimensionality where possible to make comparisons fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models with comparable dimensions\n",
    "DIM = 1024  # Common dimension for FHRR and MAP\n",
    "\n",
    "models = {\n",
    "    \"FHRR\": create_fhrr_model(dim=DIM),\n",
    "    \"MAP\": create_map_model(dim=DIM),\n",
    "    \"Binary\": create_binary_model(dim=DIM * 10, bipolar=True),  # Binary needs higher dim\n",
    "}\n",
    "\n",
    "# Create memories for each model\n",
    "memories = {name: VSAMemory(model) for name, model in models.items()}\n",
    "\n",
    "print(\"Models created:\")\n",
    "for name, model in models.items():\n",
    "    print(f\"  {name:8s}: {model.dim:5d} dimensions, {model.rep_cls.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Classification Performance (Iris Dataset)\n",
    "\n",
    "Let's compare how well each model performs on a simple classification task using the Iris dataset.\n",
    "\n",
    "**Approach**: Prototype-based classification\n",
    "1. Encode features as VSA vectors\n",
    "2. Build class prototypes from training examples\n",
    "3. Classify test samples by similarity to prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "feature_names = iris.feature_names\n",
    "class_names = iris.target_names\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {len(X_train)} training samples, {len(X_test)} test samples\")\n",
    "print(f\"Features: {feature_names}\")\n",
    "print(f\"Classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sample(model, memory, feature_values: np.ndarray, feature_names: List[str]) -> jnp.ndarray:\n",
    "    \"\"\"Encode a sample using scalar encoding for each feature.\"\"\"\n",
    "    # Add feature names to memory if not present\n",
    "    for name in feature_names:\n",
    "        if name not in memory:\n",
    "            memory.add(name)\n",
    "    \n",
    "    # Encode each feature: bind(feature_name, feature_value)\n",
    "    encoded_features = []\n",
    "    for name, value in zip(feature_names, feature_values):\n",
    "        # Use power encoding: feature_basis ** normalized_value\n",
    "        feature_vec = memory[name].vec\n",
    "        # Normalize value to [0, 1] range for this dataset\n",
    "        normalized_value = float(value) / 10.0  # Simple normalization\n",
    "        \n",
    "        # Power encoding (works for complex and real)\n",
    "        if hasattr(feature_vec, 'dtype') and jnp.issubdtype(feature_vec.dtype, jnp.complexfloating):\n",
    "            # For complex: rotate phase\n",
    "            encoded = feature_vec * jnp.exp(1j * normalized_value)\n",
    "        else:\n",
    "            # For real/binary: iterative binding approximation\n",
    "            encoded = feature_vec * (1 + 0.1 * normalized_value)  # Simple scaling\n",
    "        \n",
    "        encoded_features.append(encoded)\n",
    "    \n",
    "    # Bundle all features\n",
    "    result = encoded_features[0]\n",
    "    for feat in encoded_features[1:]:\n",
    "        result = result + feat\n",
    "    \n",
    "    # Normalize\n",
    "    return result / jnp.linalg.norm(result)\n",
    "\n",
    "\n",
    "def build_prototypes(model, memory, X_train, y_train, feature_names, num_classes):\n",
    "    \"\"\"Build class prototypes by bundling training examples.\"\"\"\n",
    "    prototypes = {}\n",
    "    \n",
    "    for class_id in range(num_classes):\n",
    "        # Get all samples for this class\n",
    "        class_samples = X_train[y_train == class_id]\n",
    "        \n",
    "        # Encode and bundle\n",
    "        encoded_samples = [\n",
    "            encode_sample(model, memory, sample, feature_names)\n",
    "            for sample in class_samples\n",
    "        ]\n",
    "        \n",
    "        # Bundle all samples for this class\n",
    "        prototype = sum(encoded_samples) / len(encoded_samples)\n",
    "        prototype = prototype / jnp.linalg.norm(prototype)\n",
    "        prototypes[class_id] = prototype\n",
    "    \n",
    "    return prototypes\n",
    "\n",
    "\n",
    "def classify_sample(model, memory, sample, prototypes, feature_names):\n",
    "    \"\"\"Classify a sample by finding most similar prototype.\"\"\"\n",
    "    encoded = encode_sample(model, memory, sample, feature_names)\n",
    "    \n",
    "    best_class = None\n",
    "    best_sim = -float('inf')\n",
    "    \n",
    "    for class_id, prototype in prototypes.items():\n",
    "        sim = float(cosine_similarity(encoded, prototype))\n",
    "        if sim > best_sim:\n",
    "            best_sim = sim\n",
    "            best_class = class_id\n",
    "    \n",
    "    return best_class, best_sim\n",
    "\n",
    "print(\"Classification functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare classification accuracy across models\n",
    "print(\"=\" * 70)\n",
    "print(\"CLASSIFICATION ACCURACY COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    memory = memories[model_name]\n",
    "    \n",
    "    # Build prototypes\n",
    "    prototypes = build_prototypes(\n",
    "        model, memory, X_train, y_train, feature_names, len(class_names)\n",
    "    )\n",
    "    \n",
    "    # Classify test samples\n",
    "    predictions = []\n",
    "    for sample in X_test:\n",
    "        pred_class, _ = classify_sample(model, memory, sample, prototypes, feature_names)\n",
    "        predictions.append(pred_class)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(np.array(predictions) == y_test)\n",
    "    results[model_name] = accuracy\n",
    "    \n",
    "    print(f\"\\n{model_name} Model:\")\n",
    "    print(f\"  Accuracy: {accuracy:.1%} ({int(accuracy * len(y_test))}/{len(y_test)} correct)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"WINNER:\", max(results, key=results.get), f\"({results[max(results, key=results.get)]:.1%})\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Noise Robustness\n",
    "\n",
    "How well can each model recover from noisy representations?\n",
    "\n",
    "**Test**: Add increasing amounts of random noise to a vector, measure similarity to original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_noise_robustness(model, memory, noise_levels):\n",
    "    \"\"\"Test how well a model recovers from noise.\"\"\"\n",
    "    # Create a test vector\n",
    "    memory.add(\"test_concept\")\n",
    "    original = memory[\"test_concept\"].vec\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for noise_level in noise_levels:\n",
    "        # Add Gaussian noise\n",
    "        if jnp.issubdtype(original.dtype, jnp.complexfloating):\n",
    "            noise = (np.random.randn(model.dim) + 1j * np.random.randn(model.dim)) * noise_level\n",
    "        else:\n",
    "            noise = np.random.randn(model.dim) * noise_level\n",
    "        \n",
    "        noisy = original + noise\n",
    "        noisy = noisy / jnp.linalg.norm(noisy)  # Renormalize\n",
    "        \n",
    "        # Measure similarity to original\n",
    "        similarity = float(cosine_similarity(original, noisy))\n",
    "        results.append(similarity)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test noise robustness\n",
    "noise_levels = [0.0, 0.1, 0.2, 0.3, 0.5, 0.7, 1.0, 1.5, 2.0]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"NOISE ROBUSTNESS TEST\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nSimilarity to original after adding noise:\\n\")\n",
    "\n",
    "noise_results = {}\n",
    "for model_name, model in models.items():\n",
    "    # Create fresh memory for this test\n",
    "    memory = VSAMemory(model)\n",
    "    results = test_noise_robustness(model, memory, noise_levels)\n",
    "    noise_results[model_name] = results\n",
    "\n",
    "# Print results as table\n",
    "print(f\"{'Noise':>8s}\", end=\"\")\n",
    "for model_name in models.keys():\n",
    "    print(f\"  {model_name:>8s}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, noise_level in enumerate(noise_levels):\n",
    "    print(f\"{noise_level:>8.2f}\", end=\"\")\n",
    "    for model_name in models.keys():\n",
    "        sim = noise_results[model_name][i]\n",
    "        print(f\"  {sim:>8.3f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Most noise-robust: Look for highest similarity at high noise levels\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Capacity Analysis\n",
    "\n",
    "How many items can we bundle before they start interfering with each other?\n",
    "\n",
    "**Test**: Bundle increasing numbers of random vectors, try to retrieve each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_capacity(model, memory, max_items=50, step=5):\n",
    "    \"\"\"Test bundling capacity by measuring retrieval accuracy.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for n_items in range(step, max_items + 1, step):\n",
    "        # Create n random items\n",
    "        items = []\n",
    "        for i in range(n_items):\n",
    "            name = f\"item_{i}\"\n",
    "            if name not in memory:\n",
    "                memory.add(name)\n",
    "            items.append(memory[name].vec)\n",
    "        \n",
    "        # Bundle all items\n",
    "        bundle = sum(items) / len(items)\n",
    "        bundle = bundle / jnp.linalg.norm(bundle)\n",
    "        \n",
    "        # Try to retrieve each item from the bundle\n",
    "        similarities = []\n",
    "        for item in items:\n",
    "            sim = float(cosine_similarity(bundle, item))\n",
    "            similarities.append(sim)\n",
    "        \n",
    "        # Average similarity\n",
    "        avg_sim = np.mean(similarities)\n",
    "        results.append((n_items, avg_sim))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test capacity\n",
    "print(\"=\" * 70)\n",
    "print(\"CAPACITY TEST: Bundling Interference\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nAverage similarity to bundled items:\\n\")\n",
    "\n",
    "capacity_results = {}\n",
    "for model_name, model in models.items():\n",
    "    memory = VSAMemory(model)\n",
    "    results = test_capacity(model, memory, max_items=50, step=10)\n",
    "    capacity_results[model_name] = results\n",
    "\n",
    "# Print results\n",
    "print(f\"{'Items':>8s}\", end=\"\")\n",
    "for model_name in models.keys():\n",
    "    print(f\"  {model_name:>8s}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * 70)\n",
    "\n",
    "n_steps = len(capacity_results[list(models.keys())[0]])\n",
    "for i in range(n_steps):\n",
    "    n_items = capacity_results[list(models.keys())[0]][i][0]\n",
    "    print(f\"{n_items:>8d}\", end=\"\")\n",
    "    for model_name in models.keys():\n",
    "        sim = capacity_results[model_name][i][1]\n",
    "        print(f\"  {sim:>8.3f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Higher similarity = better capacity (less interference)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Speed Benchmark\n",
    "\n",
    "Compare execution speed for common operations: sampling, binding, bundling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_operation(model, operation, n_trials=100):\n",
    "    \"\"\"Benchmark an operation.\"\"\"\n",
    "    # Create test vectors\n",
    "    memory = VSAMemory(model)\n",
    "    memory.add_many([f\"vec_{i}\" for i in range(10)])\n",
    "    \n",
    "    vectors = [memory[f\"vec_{i}\"].vec for i in range(10)]\n",
    "    \n",
    "    # Warm-up (for JIT compilation)\n",
    "    if operation == \"bind\":\n",
    "        _ = model.opset.bind(vectors[0], vectors[1])\n",
    "    elif operation == \"bundle\":\n",
    "        _ = model.opset.bundle(*vectors)\n",
    "    elif operation == \"sample\":\n",
    "        _ = model.sampler(model.dim, 1)\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.time()\n",
    "    for _ in range(n_trials):\n",
    "        if operation == \"bind\":\n",
    "            _ = model.opset.bind(vectors[0], vectors[1])\n",
    "        elif operation == \"bundle\":\n",
    "            _ = model.opset.bundle(*vectors)\n",
    "        elif operation == \"sample\":\n",
    "            _ = model.sampler(model.dim, 1)\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    return elapsed / n_trials * 1000  # ms per operation\n",
    "\n",
    "\n",
    "# Benchmark all models\n",
    "print(\"=\" * 70)\n",
    "print(\"SPEED BENCHMARK (milliseconds per operation)\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "operations = [\"sample\", \"bind\", \"bundle\"]\n",
    "speed_results = {op: {} for op in operations}\n",
    "\n",
    "for operation in operations:\n",
    "    print(f\"{operation.upper()} operation:\")\n",
    "    for model_name, model in models.items():\n",
    "        time_ms = benchmark_operation(model, operation, n_trials=100)\n",
    "        speed_results[operation][model_name] = time_ms\n",
    "        print(f\"  {model_name:8s}: {time_ms:8.4f} ms\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Lower is better (faster)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Decision Guide\n",
    "\n",
    "Based on our comprehensive comparison, here's when to use each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DECISION GUIDE: Which VSA Model Should You Use?\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"ðŸŒŸ FHRR (Complex Hypervectors)\")\n",
    "print(\"   âœ“ Best for: Semantic similarity, analogies, NLP tasks\")\n",
    "print(\"   âœ“ Strengths: Exact unbinding, phase-based encoding\")\n",
    "print(\"   âœ— Drawbacks: Higher memory (complex numbers)\")\n",
    "print(\"   ðŸ“Š Use when: Accuracy matters most, semantic reasoning\")\n",
    "print()\n",
    "print(\"âš¡ MAP (Real Hypervectors)\")\n",
    "print(\"   âœ“ Best for: Fast prototyping, interpretable features\")\n",
    "print(\"   âœ“ Strengths: Simple operations, real-valued (interpretable)\")\n",
    "print(\"   âœ— Drawbacks: Approximate unbinding\")\n",
    "print(\"   ðŸ“Š Use when: Speed matters, don't need exact retrieval\")\n",
    "print()\n",
    "print(\"ðŸ’¾ Binary (Discrete Hypervectors)\")\n",
    "print(\"   âœ“ Best for: Hardware implementations, memory efficiency\")\n",
    "print(\"   âœ“ Strengths: Exact unbinding, 1-bit storage, XOR is fast\")\n",
    "print(\"   âœ— Drawbacks: Needs higher dimensions (~10x)\")\n",
    "print(\"   ðŸ“Š Use when: Deploying to hardware, memory constrained\")\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"General Rule: Start with FHRR, switch to MAP for speed,\")\n",
    "print(\"              use Binary for hardware/embedded systems\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Classification**: All three models achieve good accuracy on structured data\n",
    "2. **Noise Robustness**: FHRR and Binary maintain similarity better under noise\n",
    "3. **Capacity**: Higher dimensions â†’ more capacity; bundling degrades similarity\n",
    "4. **Speed**: MAP is typically fastest; FHRR uses FFT (still fast); Binary simple but needs more dims\n",
    "5. **Trade-offs**: Accuracy vs Speed vs Memory - choose based on your constraints\n",
    "\n",
    "## Model Selection Checklist\n",
    "\n",
    "Ask yourself:\n",
    "- **Do I need exact unbinding?** â†’ FHRR or Binary\n",
    "- **Is speed critical?** â†’ MAP\n",
    "- **Am I doing NLP/semantic tasks?** â†’ FHRR\n",
    "- **Deploying to hardware?** â†’ Binary\n",
    "- **Need interpretable real-valued vectors?** â†’ MAP\n",
    "- **Memory constrained?** â†’ Binary (1 bit per dimension)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try these benchmarks with your own data\n",
    "- Experiment with different dimensions\n",
    "- Test on your specific use case\n",
    "- Explore hybrid approaches (combine models for different tasks)\n",
    "\n",
    "## References\n",
    "\n",
    "- Plate, T. A. (1995). \"Holographic Reduced Representations\" (FHRR)\n",
    "- Gayler, R. W. (1998). \"Multiplicative Binding, Representation Operators, and Analogy\" (MAP)\n",
    "- Kanerva, P. (2009). \"Hyperdimensional Computing\" (Binary Spatter Codes)\n",
    "- Kleyko et al. (2021). \"A Survey on Hyperdimensional Computing\"\n",
    "\n",
    "## Running This Tutorial\n",
    "\n",
    "```bash\n",
    "jupyter notebook examples/notebooks/tutorial_05_model_comparison.ipynb\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
