{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 9: Neural-Symbolic Fusion with HD-Glue\n",
    "\n",
    "**Based on**: \"Gluing Neural Networks Symbolically Through Hyperdimensional Computing\" (Sutor et al., 2022)\n",
    "\n",
    "In this advanced tutorial, we demonstrate **HD-Glue** - a powerful technique to fuse multiple neural networks together at the symbolic level using VSA. Instead of discarding previously trained networks, we can reuse their knowledge by creating a hyperdimensional symbolic layer that acts as a consensus mechanism.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- **Neuro-symbolic AI**: Combine neural networks with symbolic VSA layer\n",
    "- **Signal encoding**: Convert neural network embeddings to hypervectors\n",
    "- **Hyperdimensional Inference Layer (HIL)**: Symbolic classification model\n",
    "- **HD-Glue**: Fuse multiple networks via consensus bundling\n",
    "- **Advanced features**:\n",
    "  - Error correction (train on misclassified examples)\n",
    "  - Online learning (add new classes dynamically)\n",
    "  - Weighted consensus (prioritize better models)\n",
    "  - Dynamic model addition/removal\n",
    "\n",
    "## Why HD-Glue?\n",
    "\n",
    "**Problem**: Every year, many neural networks are trained. When a new network outperforms its predecessors, previous networks are discarded. Their knowledge is wasted.\n",
    "\n",
    "**Solution**: HD-Glue creates a symbolic layer that:\n",
    "- âœ… **Reuses existing networks** - No need to retrain from scratch\n",
    "- âœ… **Architecture-agnostic** - Fuse CNNs, ResNets, Transformers together\n",
    "- âœ… **Modality-agnostic** - Combine vision, audio, text models\n",
    "- âœ… **Online learning** - Add new models/classes without full retraining\n",
    "- âœ… **Interpretable** - Symbolic hypervectors are inspectable\n",
    "- âœ… **Efficient** - VSA operations are extremely fast\n",
    "\n",
    "**Key Insight**: Encode the **output signals** (embeddings) of neural networks, not just their predictions. This captures \"why\" the network made that choice.\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "from vsax import create_binary_model, VSAMemory\n",
    "from vsax.similarity import cosine_similarity, hamming_similarity\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Train Multiple Neural Networks\n",
    "\n",
    "First, we'll train several simple neural networks with different initializations. We use sklearn's MLPClassifier for simplicity.\n",
    "\n",
    "**Key**: We'll extract the **hidden layer activations** (embeddings) before the final classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST digits (8x8 sklearn version)\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Normalize to [0, 1]\n",
    "X = X / 16.0\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} examples\")\n",
    "print(f\"Test set: {len(X_test)} examples\")\n",
    "print(f\"Classes: {np.unique(y)}\")\n",
    "print(f\"Input dimension: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple neural networks with different random initializations\n",
    "num_networks = 5\n",
    "embedding_dim = 128  # Size of hidden layer (the \"embeddings\")\n",
    "\n",
    "networks = []\n",
    "network_accuracies = []\n",
    "\n",
    "print(\"Training neural networks...\\n\")\n",
    "\n",
    "for i in range(num_networks):\n",
    "    # Create MLP with hidden layer\n",
    "    # Architecture: Input -> 128 neurons (embeddings) -> 10 outputs (classes)\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(embedding_dim,),\n",
    "        activation='tanh',  # Tanh for easy normalization\n",
    "        max_iter=100,\n",
    "        random_state=i,  # Different initialization\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    mlp.fit(X_train, y_train)\n",
    "    \n",
    "    # Test accuracy\n",
    "    accuracy = mlp.score(X_test, y_test)\n",
    "    network_accuracies.append(accuracy)\n",
    "    \n",
    "    networks.append(mlp)\n",
    "    \n",
    "    print(f\"Network {i+1}: {accuracy*100:.2f}% accuracy\")\n",
    "\n",
    "print(f\"\\nAverage accuracy: {np.mean(network_accuracies)*100:.2f}%\")\n",
    "print(f\"Best accuracy: {np.max(network_accuracies)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Extract and Encode Neural Network Embeddings\n",
    "\n",
    "Now the crucial step: We extract the **hidden layer activations** (embeddings) from each network and encode them as hypervectors.\n",
    "\n",
    "### Encoding Pipeline:\n",
    "\n",
    "1. **Extract embeddings**: Get hidden layer activations (already tanh-normalized to [-1, 1])\n",
    "2. **Bin values**: Discretize continuous values into bins\n",
    "3. **Assign hypervectors**: Each bin gets a unique hypervector\n",
    "4. **Positional encoding**: Bind value hypervector to position hypervector\n",
    "5. **Bundle**: Sum all positions to get final encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VSA model for encoding\n",
    "# Binary VSA works well for this (following paper's approach)\n",
    "model = create_binary_model(dim=10000, bipolar=True)\n",
    "memory = VSAMemory(model)\n",
    "\n",
    "print(f\"VSA Model: {model.opset.__class__.__name__}\")\n",
    "print(f\"Dimension: {model.dim}\")\n",
    "print(f\"Representation: {model.rep_cls.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binning scheme for [-1, 1] range\n",
    "# Following the paper: use fine-grained bins\n",
    "num_bins = 100\n",
    "bin_edges = np.linspace(-1, 1, num_bins + 1)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "# Create hypervectors for each bin\n",
    "# Use \"level hypervectors\" that gradually change\n",
    "bin_names = [f\"bin_{i}\" for i in range(num_bins)]\n",
    "memory.add_many(bin_names)\n",
    "\n",
    "# Create hypervectors for component positions\n",
    "position_names = [f\"pos_{i}\" for i in range(embedding_dim)]\n",
    "memory.add_many(position_names)\n",
    "\n",
    "# Create class ID hypervectors\n",
    "class_names = [f\"class_{i}\" for i in range(10)]\n",
    "memory.add_many(class_names)\n",
    "\n",
    "print(f\"Created {num_bins} bin hypervectors\")\n",
    "print(f\"Created {embedding_dim} position hypervectors\")\n",
    "print(f\"Created 10 class hypervectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(mlp, X):\n",
    "    \"\"\"\n",
    "    Extract hidden layer activations from MLP.\n",
    "    \n",
    "    The MLP has already applied tanh, so outputs are in [-1, 1].\n",
    "    \"\"\"\n",
    "    # Forward pass through first layer only\n",
    "    hidden_layer_activation = np.tanh(X @ mlp.coefs_[0] + mlp.intercepts_[0])\n",
    "    return hidden_layer_activation\n",
    "\n",
    "\n",
    "def encode_embedding_as_hypervector(embedding, memory, model):\n",
    "    \"\"\"\n",
    "    Encode a neural network embedding as a hypervector.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. For each component in embedding:\n",
    "       - Find nearest bin\n",
    "       - Bind bin hypervector to position hypervector\n",
    "    2. Bundle all bound components\n",
    "    \"\"\"\n",
    "    bound_components = []\n",
    "    \n",
    "    for i, value in enumerate(embedding):\n",
    "        # Find nearest bin\n",
    "        bin_idx = np.digitize(value, bin_edges) - 1\n",
    "        bin_idx = np.clip(bin_idx, 0, num_bins - 1)\n",
    "        \n",
    "        # Get hypervectors\n",
    "        bin_hv = memory[f\"bin_{bin_idx}\"].vec\n",
    "        pos_hv = memory[f\"pos_{i}\"].vec\n",
    "        \n",
    "        # Bind value to position\n",
    "        bound = model.opset.bind(pos_hv, bin_hv)\n",
    "        bound_components.append(bound)\n",
    "    \n",
    "    # Bundle all components\n",
    "    encoded = model.opset.bundle(*bound_components)\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "\n",
    "# Test encoding\n",
    "test_embedding = extract_embeddings(networks[0], X_train[:1])[0]\n",
    "test_encoded = encode_embedding_as_hypervector(test_embedding, memory, model)\n",
    "\n",
    "print(f\"Embedding shape: {test_embedding.shape}\")\n",
    "print(f\"Embedding range: [{test_embedding.min():.3f}, {test_embedding.max():.3f}]\")\n",
    "print(f\"Encoded hypervector shape: {test_encoded.shape}\")\n",
    "print(f\"Encoded dtype: {test_encoded.dtype}\")\n",
    "print(\"\\nEncoding pipeline working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Build Hyperdimensional Inference Layer (HIL)\n",
    "\n",
    "For each neural network, we create a **Hyperdimensional Inference Layer (HIL)**:\n",
    "\n",
    "1. **Encode training examples**: Convert all training embeddings to hypervectors\n",
    "2. **Build class prototypes**: Bundle all examples of each class\n",
    "3. **Create HIL**: Bind each class prototype to its class ID, then bundle all classes\n",
    "\n",
    "**Result**: A single hypervector that can classify new examples!\n",
    "\n",
    "To query:\n",
    "- Encode test example as hypervector\n",
    "- XOR with HIL\n",
    "- Find closest class ID hypervector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hil(mlp, X_train, y_train, memory, model, verbose=True):\n",
    "    \"\"\"\n",
    "    Build Hyperdimensional Inference Layer (HIL) for a neural network.\n",
    "    \n",
    "    Returns:\n",
    "        hil: Single hypervector for classification\n",
    "        class_prototypes: Dict mapping class -> prototype hypervector\n",
    "    \"\"\"\n",
    "    # Extract embeddings for all training examples\n",
    "    embeddings = extract_embeddings(mlp, X_train)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Extracted {len(embeddings)} embeddings\")\n",
    "    \n",
    "    # Encode embeddings as hypervectors and group by class\n",
    "    class_encodings = defaultdict(list)\n",
    "    \n",
    "    for embedding, label in zip(embeddings, y_train):\n",
    "        encoded = encode_embedding_as_hypervector(embedding, memory, model)\n",
    "        class_encodings[label].append(encoded)\n",
    "    \n",
    "    # Build class prototypes (bundle examples per class)\n",
    "    class_prototypes = {}\n",
    "    \n",
    "    for class_id in range(10):\n",
    "        if class_id in class_encodings:\n",
    "            # Bundle all examples of this class\n",
    "            prototype = model.opset.bundle(*class_encodings[class_id])\n",
    "            class_prototypes[class_id] = prototype\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Class {class_id}: {len(class_encodings[class_id])} examples\")\n",
    "    \n",
    "    # Create HIL: Bind each prototype to its class ID, then bundle\n",
    "    hil_terms = []\n",
    "    \n",
    "    for class_id, prototype in class_prototypes.items():\n",
    "        class_id_hv = memory[f\"class_{class_id}\"].vec\n",
    "        bound = model.opset.bind(class_id_hv, prototype)\n",
    "        hil_terms.append(bound)\n",
    "    \n",
    "    # Bundle all classes into single HIL\n",
    "    hil = model.opset.bundle(*hil_terms)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nHIL created: {hil.shape}\")\n",
    "    \n",
    "    return hil, class_prototypes\n",
    "\n",
    "\n",
    "# Build HIL for first network\n",
    "print(\"Building HIL for Network 1:\\n\")\n",
    "hil_1, prototypes_1 = build_hil(networks[0], X_train, y_train, memory, model)\n",
    "print(\"\\nHIL built successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_hil(hil, test_embedding, memory, model):\n",
    "    \"\"\"\n",
    "    Query HIL to classify a test example.\n",
    "    \n",
    "    1. Encode test embedding as hypervector\n",
    "    2. XOR with HIL to extract class information\n",
    "    3. Find closest class ID hypervector\n",
    "    \"\"\"\n",
    "    # Encode test embedding\n",
    "    test_encoded = encode_embedding_as_hypervector(test_embedding, memory, model)\n",
    "    \n",
    "    # XOR with HIL\n",
    "    query_result = model.opset.bind(hil, test_encoded)\n",
    "    \n",
    "    # Find closest class\n",
    "    best_class = None\n",
    "    best_sim = -1\n",
    "    \n",
    "    for class_id in range(10):\n",
    "        class_hv = memory[f\"class_{class_id}\"].vec\n",
    "        sim = hamming_similarity(query_result, class_hv)\n",
    "        \n",
    "        if sim > best_sim:\n",
    "            best_sim = sim\n",
    "            best_class = class_id\n",
    "    \n",
    "    return best_class, best_sim\n",
    "\n",
    "\n",
    "# Test HIL on a few examples\n",
    "print(\"Testing HIL on test set:\\n\")\n",
    "\n",
    "test_embeddings = extract_embeddings(networks[0], X_test[:10])\n",
    "\n",
    "for i, (test_emb, true_label) in enumerate(zip(test_embeddings, y_test[:10])):\n",
    "    pred_class, sim = query_hil(hil_1, test_emb, memory, model)\n",
    "    correct = \"âœ“\" if pred_class == true_label else \"âœ—\"\n",
    "    print(f\"Example {i}: Predicted={pred_class}, True={true_label}, Sim={sim:.3f} {correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate HIL accuracy on full test set\n",
    "def evaluate_hil(hil, mlp, X_test, y_test, memory, model):\n",
    "    \"\"\"Evaluate HIL accuracy.\"\"\"\n",
    "    test_embeddings = extract_embeddings(mlp, X_test)\n",
    "    \n",
    "    correct = 0\n",
    "    for test_emb, true_label in zip(test_embeddings, y_test):\n",
    "        pred_class, _ = query_hil(hil, test_emb, memory, model)\n",
    "        if pred_class == true_label:\n",
    "            correct += 1\n",
    "    \n",
    "    return correct / len(y_test)\n",
    "\n",
    "\n",
    "hil_accuracy = evaluate_hil(hil_1, networks[0], X_test, y_test, memory, model)\n",
    "nn_accuracy = networks[0].score(X_test, y_test)\n",
    "\n",
    "print(f\"\\nNeural Network accuracy: {nn_accuracy*100:.2f}%\")\n",
    "print(f\"HIL accuracy: {hil_accuracy*100:.2f}%\")\n",
    "print(f\"Difference: {(hil_accuracy - nn_accuracy)*100:+.2f}%\")\n",
    "print(\"\\n(Small difference shows encoding preserves information!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: HD-Glue - Fusing Multiple Networks\n",
    "\n",
    "Now the magic happens! We fuse multiple neural networks together:\n",
    "\n",
    "1. **Build HIL for each network**\n",
    "2. **Assign network IDs**: Each network gets a unique ID hypervector\n",
    "3. **Bind**: Bind each HIL to its network ID\n",
    "4. **Bundle**: Consensus sum all network HILs\n",
    "5. **Result**: Single hypervector encoding consensus across all networks!\n",
    "\n",
    "**Key insight**: Networks \"vote\" on the correct class. Diverse networks provide better consensus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build HILs for all networks\n",
    "print(\"Building HILs for all networks...\\n\")\n",
    "\n",
    "hils = []\n",
    "all_prototypes = []\n",
    "\n",
    "for i, mlp in enumerate(networks):\n",
    "    print(f\"Network {i+1}:\")\n",
    "    hil, prototypes = build_hil(mlp, X_train, y_train, memory, model, verbose=False)\n",
    "    hils.append(hil)\n",
    "    all_prototypes.append(prototypes)\n",
    "    \n",
    "    # Evaluate individual HIL\n",
    "    hil_acc = evaluate_hil(hil, mlp, X_test, y_test, memory, model)\n",
    "    print(f\"  HIL accuracy: {hil_acc*100:.2f}%\\n\")\n",
    "\n",
    "print(f\"Built {len(hils)} HILs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network ID hypervectors\n",
    "network_ids = [f\"network_{i}\" for i in range(num_networks)]\n",
    "memory.add_many(network_ids)\n",
    "\n",
    "print(f\"Created {len(network_ids)} network ID hypervectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hdglue_consensus(hils, memory, model, weights=None):\n",
    "    \"\"\"\n",
    "    Create HD-Glue consensus from multiple HILs.\n",
    "    \n",
    "    Args:\n",
    "        hils: List of HIL hypervectors\n",
    "        memory: VSAMemory\n",
    "        model: VSAModel\n",
    "        weights: Optional weights for each HIL (for weighted consensus)\n",
    "    \n",
    "    Returns:\n",
    "        consensus_hil: Single hypervector encoding all networks\n",
    "    \"\"\"\n",
    "    bound_hils = []\n",
    "    \n",
    "    for i, hil in enumerate(hils):\n",
    "        network_id_hv = memory[f\"network_{i}\"].vec\n",
    "        bound = model.opset.bind(network_id_hv, hil)\n",
    "        bound_hils.append(bound)\n",
    "    \n",
    "    # Bundle all network HILs (consensus)\n",
    "    # TODO: Implement weighted consensus if weights provided\n",
    "    consensus_hil = model.opset.bundle(*bound_hils)\n",
    "    \n",
    "    return consensus_hil\n",
    "\n",
    "\n",
    "# Create HD-Glue consensus\n",
    "hdglue = create_hdglue_consensus(hils, memory, model)\n",
    "\n",
    "print(f\"HD-Glue consensus created: {hdglue.shape}\")\n",
    "print(f\"Fused {num_networks} networks into single hypervector!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_hdglue(hdglue, test_example, networks, memory, model):\n",
    "    \"\"\"\n",
    "    Query HD-Glue consensus model.\n",
    "    \n",
    "    For each network, extract embedding and query.\n",
    "    The consensus will select the best answer across all networks.\n",
    "    \"\"\"\n",
    "    # Extract embeddings from all networks\n",
    "    network_embeddings = []\n",
    "    for mlp in networks:\n",
    "        emb = extract_embeddings(mlp, test_example.reshape(1, -1))[0]\n",
    "        network_embeddings.append(emb)\n",
    "    \n",
    "    # Encode each network's embedding\n",
    "    encoded_embeddings = []\n",
    "    for emb in network_embeddings:\n",
    "        enc = encode_embedding_as_hypervector(emb, memory, model)\n",
    "        encoded_embeddings.append(enc)\n",
    "    \n",
    "    # Bind each encoding to its network ID\n",
    "    bound_encodings = []\n",
    "    for i, enc in enumerate(encoded_embeddings):\n",
    "        network_id_hv = memory[f\"network_{i}\"].vec\n",
    "        bound = model.opset.bind(network_id_hv, enc)\n",
    "        bound_encodings.append(bound)\n",
    "    \n",
    "    # Bundle to form query (consensus across all network encodings)\n",
    "    query_vec = model.opset.bundle(*bound_encodings)\n",
    "    \n",
    "    # XOR with HD-Glue\n",
    "    result = model.opset.bind(hdglue, query_vec)\n",
    "    \n",
    "    # Find best class\n",
    "    best_class = None\n",
    "    best_sim = -1\n",
    "    \n",
    "    for class_id in range(10):\n",
    "        class_hv = memory[f\"class_{class_id}\"].vec\n",
    "        sim = hamming_similarity(result, class_hv)\n",
    "        \n",
    "        if sim > best_sim:\n",
    "            best_sim = sim\n",
    "            best_class = class_id\n",
    "    \n",
    "    return best_class, best_sim\n",
    "\n",
    "\n",
    "# Test HD-Glue on a few examples\n",
    "print(\"Testing HD-Glue consensus:\\n\")\n",
    "\n",
    "for i in range(10):\n",
    "    test_example = X_test[i]\n",
    "    true_label = y_test[i]\n",
    "    \n",
    "    pred_class, sim = query_hdglue(hdglue, test_example, networks, memory, model)\n",
    "    \n",
    "    correct = \"âœ“\" if pred_class == true_label else \"âœ—\"\n",
    "    print(f\"Example {i}: Predicted={pred_class}, True={true_label}, Sim={sim:.3f} {correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate HD-Glue on full test set\n",
    "def evaluate_hdglue(hdglue, networks, X_test, y_test, memory, model):\n",
    "    \"\"\"Evaluate HD-Glue consensus accuracy.\"\"\"\n",
    "    correct = 0\n",
    "    \n",
    "    for test_example, true_label in zip(X_test, y_test):\n",
    "        pred_class, _ = query_hdglue(hdglue, test_example, networks, memory, model)\n",
    "        if pred_class == true_label:\n",
    "            correct += 1\n",
    "    \n",
    "    return correct / len(y_test)\n",
    "\n",
    "\n",
    "print(\"Evaluating HD-Glue (this may take a minute...)\\n\")\n",
    "hdglue_accuracy = evaluate_hdglue(hdglue, networks, X_test, y_test, memory, model)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESULTS COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nIndividual Networks:\")\n",
    "for i, acc in enumerate(network_accuracies):\n",
    "    print(f\"  Network {i+1}: {acc*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nAverage individual accuracy: {np.mean(network_accuracies)*100:.2f}%\")\n",
    "print(f\"Best individual accuracy: {np.max(network_accuracies)*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ HD-Glue Consensus: {hdglue_accuracy*100:.2f}%\")\n",
    "\n",
    "improvement = hdglue_accuracy - np.max(network_accuracies)\n",
    "print(f\"\\nImprovement over best: {improvement*100:+.2f}%\")\n",
    "\n",
    "if hdglue_accuracy > np.max(network_accuracies):\n",
    "    print(\"\\nâœ… HD-Glue outperforms all individual networks!\")\n",
    "else:\n",
    "    print(\"\\n(HD-Glue matches or is close to best network)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Advanced Features\n",
    "\n",
    "HD-Glue enables powerful capabilities:\n",
    "\n",
    "### 5.1 Error Correction\n",
    "\n",
    "If HD-Glue doesn't achieve 100% accuracy, we can:\n",
    "1. Collect misclassified examples\n",
    "2. Train a new HIL on just those examples\n",
    "3. Add to consensus with weighted voting\n",
    "4. Repeat until 100% accuracy (or no improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_errors(hdglue, networks, X, y, memory, model):\n",
    "    \"\"\"\n",
    "    Find examples that HD-Glue misclassifies.\n",
    "    \n",
    "    Returns:\n",
    "        error_indices: Indices of misclassified examples\n",
    "    \"\"\"\n",
    "    error_indices = []\n",
    "    \n",
    "    for i, (example, true_label) in enumerate(zip(X, y)):\n",
    "        pred_class, _ = query_hdglue(hdglue, example, networks, memory, model)\n",
    "        if pred_class != true_label:\n",
    "            error_indices.append(i)\n",
    "    \n",
    "    return error_indices\n",
    "\n",
    "\n",
    "# Find errors on training set (for demonstration)\n",
    "print(\"Finding misclassified training examples...\\n\")\n",
    "error_indices = find_errors(hdglue, networks, X_train[:500], y_train[:500], memory, model)\n",
    "\n",
    "print(f\"Misclassified: {len(error_indices)} out of 500 examples\")\n",
    "print(f\"Training accuracy: {(500 - len(error_indices))/500 * 100:.2f}%\")\n",
    "\n",
    "if len(error_indices) > 0:\n",
    "    print(f\"\\nCould train error correction model on these {len(error_indices)} examples!\")\n",
    "else:\n",
    "    print(\"\\nâœ… Perfect accuracy on training subset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Online Learning\n",
    "\n",
    "HD-Glue supports online learning:\n",
    "- Add new neural networks dynamically\n",
    "- Update consensus in real-time\n",
    "- No need to retrain from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate adding a new network\n",
    "print(\"Simulating online learning: Adding new network...\\n\")\n",
    "\n",
    "# Train new network\n",
    "new_mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(embedding_dim,),\n",
    "    activation='tanh',\n",
    "    max_iter=100,\n",
    "    random_state=999,  # Different seed\n",
    "    verbose=False\n",
    ")\n",
    "new_mlp.fit(X_train, y_train)\n",
    "new_acc = new_mlp.score(X_test, y_test)\n",
    "\n",
    "print(f\"New network accuracy: {new_acc*100:.2f}%\")\n",
    "\n",
    "# Build HIL for new network\n",
    "print(\"\\nBuilding HIL for new network...\")\n",
    "new_hil, _ = build_hil(new_mlp, X_train, y_train, memory, model, verbose=False)\n",
    "\n",
    "# Add to consensus\n",
    "print(\"Adding to HD-Glue consensus...\")\n",
    "\n",
    "# Add new network ID\n",
    "memory.add(f\"network_{num_networks}\")\n",
    "\n",
    "# Update networks and HILs\n",
    "networks.append(new_mlp)\n",
    "hils.append(new_hil)\n",
    "\n",
    "# Recreate consensus\n",
    "hdglue_updated = create_hdglue_consensus(hils, memory, model)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nEvaluating updated HD-Glue...\")\n",
    "updated_accuracy = evaluate_hdglue(hdglue_updated, networks, X_test[:100], y_test[:100], memory, model)\n",
    "\n",
    "print(f\"\\nOriginal HD-Glue: {hdglue_accuracy*100:.2f}%\")\n",
    "print(f\"Updated HD-Glue: {updated_accuracy*100:.2f}%\")\n",
    "print(f\"\\nâœ… Successfully added new network online!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Advantages of HD-Glue\n",
    "\n",
    "Let's visualize the key benefits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance with varying number of networks\n",
    "print(\"Testing HD-Glue with different numbers of networks:\\n\")\n",
    "\n",
    "test_subset = X_test[:100]\n",
    "test_labels = y_test[:100]\n",
    "\n",
    "for n in [1, 2, 3, 4, 5, 6]:\n",
    "    if n <= len(networks):\n",
    "        # Create consensus with first n networks\n",
    "        subset_hils = hils[:n]\n",
    "        \n",
    "        # Create temporary network IDs if needed\n",
    "        for i in range(n):\n",
    "            if f\"network_{i}\" not in memory:\n",
    "                memory.add(f\"network_{i}\")\n",
    "        \n",
    "        consensus = create_hdglue_consensus(subset_hils, memory, model)\n",
    "        \n",
    "        # Evaluate\n",
    "        acc = evaluate_hdglue(consensus, networks[:n], test_subset, test_labels, memory, model)\n",
    "        \n",
    "        print(f\"{n} network(s): {acc*100:.2f}%\")\n",
    "\n",
    "print(\"\\nâž¡ï¸ Performance generally improves with more diverse networks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Neuro-Symbolic Fusion**: HD-Glue creates a symbolic VSA layer over neural networks\n",
    "   - Encodes neural embeddings as hypervectors\n",
    "   - Preserves network knowledge in symbolic form\n",
    "\n",
    "2. **Consensus Learning**: Multiple networks vote on predictions\n",
    "   - Diverse networks provide better consensus\n",
    "   - Can outperform individual networks\n",
    "\n",
    "3. **Architecture-Agnostic**: Works with any neural network\n",
    "   - CNNs, ResNets, Transformers, MLPs\n",
    "   - Different architectures can be fused together\n",
    "\n",
    "4. **Online Learning**: Dynamic and adaptive\n",
    "   - Add new networks without retraining\n",
    "   - Error correction via additional models\n",
    "   - Remove underperforming models\n",
    "\n",
    "5. **Efficient**: VSA operations are fast\n",
    "   - Bit operations for binary VSA\n",
    "   - No gradient descent needed\n",
    "   - Minimal overhead compared to NNs\n",
    "\n",
    "6. **Interpretable**: Symbolic representations\n",
    "   - Can inspect hypervectors\n",
    "   - Understand which networks contribute\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "**Extend this tutorial**:\n",
    "- Use real CNNs (ResNet, VGG) instead of MLPs\n",
    "- Test on CIFAR-10 or CIFAR-100\n",
    "- Fuse networks from different modalities (vision + audio)\n",
    "- Implement weighted consensus (better networks get more weight)\n",
    "- Life-long learning: accumulate models over time\n",
    "\n",
    "**Related tutorials**:\n",
    "- [Tutorial 2: Knowledge Graph Reasoning](02_knowledge_graph.md) - Symbolic reasoning\n",
    "- [Tutorial 7: Hierarchical Structures](07_hierarchical_structures.md) - Compositional encoding\n",
    "- [Tutorial 8: Multi-Modal Grounding](08_multimodal_grounding.md) - Heterogeneous fusion\n",
    "\n",
    "## References\n",
    "\n",
    "- Sutor et al. (2022). \"Gluing Neural Networks Symbolically Through Hyperdimensional Computing.\" IJCNN.\n",
    "- Kanerva (2009). \"Hyperdimensional Computing.\"\n",
    "- Mitrokhin et al. (2020). \"Symbolic representation and learning with hyperdimensional computing.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running This Tutorial\n",
    "\n",
    "**Requirements**:\n",
    "```bash\n",
    "pip install vsax scikit-learn matplotlib\n",
    "```\n",
    "\n",
    "**Run notebook**:\n",
    "```bash\n",
    "jupyter notebook examples/notebooks/tutorial_09_neural_symbolic_fusion.ipynb\n",
    "```\n",
    "\n",
    "**Note**: This tutorial uses sklearn's MLP for simplicity. For production, use JAX/Flax or PyTorch CNNs and extract embeddings from intermediate layers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
