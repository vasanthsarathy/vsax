{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4: Word Analogies & Random Indexing\n",
    "\n",
    "This tutorial demonstrates how to build word embeddings using **Random Indexing** and perform word analogies like the famous:\n",
    "\n",
    "**king - man + woman = queen**\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Build word embeddings from text using Random Indexing (Kanerva et al. 2000)\n",
    "- Perform word analogies using vector arithmetic\n",
    "- Find semantically similar words\n",
    "- Compare different VSA models for word representations\n",
    "- Understand how context shapes meaning\n",
    "\n",
    "## Why Random Indexing?\n",
    "\n",
    "From Kanerva et al. (2000):\n",
    "> \"Random Indexing is a word space model that accumulates context vectors based on co-occurrence data.\"\n",
    "\n",
    "**Key Idea**: Words that appear in similar contexts have similar meanings.\n",
    "\n",
    "**How it works**:\n",
    "1. Assign each word a random **index vector** (unique identifier)\n",
    "2. For each word occurrence, accumulate the index vectors of nearby words (**context**)\n",
    "3. The accumulated vector is the word's **semantic vector**\n",
    "4. Similar contexts → similar vectors\n",
    "\n",
    "**Advantages**:\n",
    "- Incremental (online learning)\n",
    "- Fixed dimensionality (no SVD needed)\n",
    "- Scalable to large corpora\n",
    "- Captures semantic relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from vsax import create_fhrr_model, create_map_model, create_binary_model\n",
    "from vsax import VSAMemory\n",
    "from vsax.similarity import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Use FHRR model (best for semantic similarity)\n",
    "model = create_fhrr_model(dim=512)\n",
    "memory = VSAMemory(model)\n",
    "\n",
    "print(f\"Model: {model.rep_cls.__name__}\")\n",
    "print(f\"Dimension: {model.dim}\")\n",
    "print(f\"Ready for Random Indexing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Sample Text Corpus\n",
    "\n",
    "We'll use a small corpus with clear semantic relationships to demonstrate the concepts.\n",
    "\n",
    "The corpus includes sentences about:\n",
    "- Royalty (kings, queens, princes, princesses)\n",
    "- Countries and capitals\n",
    "- Gender relationships\n",
    "- Family relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus with semantic relationships\n",
    "corpus = \"\"\"\n",
    "The king rules the kingdom with wisdom and strength.\n",
    "The queen stands beside the king as his equal partner.\n",
    "A prince is the son of a king and queen.\n",
    "A princess is the daughter of a king and queen.\n",
    "The king and his son the prince govern together.\n",
    "The queen and her daughter the princess lead with grace.\n",
    "\n",
    "A man can become a king through inheritance or marriage.\n",
    "A woman can become a queen through inheritance or marriage.\n",
    "The man and woman were married in the kingdom.\n",
    "Every man and woman in the kingdom celebrated.\n",
    "\n",
    "The boy grew up to become a strong man.\n",
    "The girl grew up to become a wise woman.\n",
    "A father is a man with children.\n",
    "A mother is a woman with children.\n",
    "The father and mother raised their son and daughter.\n",
    "\n",
    "Paris is the capital of France and a beautiful city.\n",
    "France is a country in Europe with Paris as its capital.\n",
    "London is the capital of England and a historic city.\n",
    "England is a country in Europe with London as its capital.\n",
    "Berlin is the capital of Germany and a vibrant city.\n",
    "Germany is a country in Europe with Berlin as its capital.\n",
    "Rome is the capital of Italy and an ancient city.\n",
    "Italy is a country in Europe with Rome as its capital.\n",
    "\n",
    "The capital city represents the country it serves.\n",
    "Every country has a capital where government resides.\n",
    "Europe contains many countries with famous capitals.\n",
    "\n",
    "A doctor helps people by treating illness and injury.\n",
    "A teacher helps people by sharing knowledge and wisdom.\n",
    "A nurse helps people by providing care and comfort.\n",
    "Doctors and nurses work together in hospitals.\n",
    "Teachers and students work together in schools.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Corpus: {len(corpus)} characters\")\n",
    "print(f\"Sample: {corpus[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> List[List[str]]:\n",
    "    \"\"\"Tokenize text into sentences and words.\"\"\"\n",
    "    # Split into sentences\n",
    "    sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "    \n",
    "    # Tokenize each sentence\n",
    "    tokenized = []\n",
    "    for sent in sentences:\n",
    "        # Convert to lowercase and extract words\n",
    "        words = re.findall(r'\\b[a-z]+\\b', sent.lower())\n",
    "        if len(words) > 0:\n",
    "            tokenized.append(words)\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Preprocess corpus\n",
    "sentences = preprocess_text(corpus)\n",
    "\n",
    "print(f\"Number of sentences: {len(sentences)}\")\n",
    "print(f\"\\nSample sentences:\")\n",
    "for i, sent in enumerate(sentences[:3]):\n",
    "    print(f\"{i+1}. {' '.join(sent)}\")\n",
    "\n",
    "# Get vocabulary\n",
    "vocabulary = set()\n",
    "for sent in sentences:\n",
    "    vocabulary.update(sent)\n",
    "\n",
    "print(f\"\\nVocabulary size: {len(vocabulary)} unique words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Random Indexing - Building Word Embeddings\n",
    "\n",
    "The Random Indexing algorithm:\n",
    "\n",
    "1. **Index Vectors**: Assign each word a random vector (its \"signature\")\n",
    "2. **Context Accumulation**: For each word occurrence, sum the index vectors of nearby words\n",
    "3. **Semantic Vectors**: The accumulated sum becomes the word's meaning\n",
    "\n",
    "**Example**: \n",
    "```\n",
    "\"The king rules the kingdom\"\n",
    "```\n",
    "For \"king\", we accumulate index vectors of: the, rules, the, kingdom\n",
    "These context words shape \"king\"'s semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index vectors (random signatures) for all words\n",
    "print(\"Creating index vectors for vocabulary...\")\n",
    "memory.add_many(list(vocabulary))\n",
    "\n",
    "print(f\"Created {len(memory)} index vectors\")\n",
    "print(f\"Each vector: {model.dim} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_semantic_vectors(sentences: List[List[str]], \n",
    "                          window_size: int = 2) -> Dict[str, jnp.ndarray]:\n",
    "    \"\"\"Build semantic vectors using Random Indexing.\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of tokenized sentences\n",
    "        window_size: Context window (words before/after to include)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping words to semantic vectors\n",
    "    \"\"\"\n",
    "    # Initialize context accumulators\n",
    "    context_vectors = defaultdict(lambda: jnp.zeros(model.dim, dtype=jnp.complex64))\n",
    "    \n",
    "    # Process each sentence\n",
    "    for sent in sentences:\n",
    "        # For each word position\n",
    "        for i, word in enumerate(sent):\n",
    "            # Get context window\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(sent), i + window_size + 1)\n",
    "            \n",
    "            # Accumulate index vectors of context words\n",
    "            for j in range(start, end):\n",
    "                if j != i:  # Don't include the word itself\n",
    "                    context_word = sent[j]\n",
    "                    context_vectors[word] = context_vectors[word] + memory[context_word].vec\n",
    "    \n",
    "    return dict(context_vectors)\n",
    "\n",
    "# Build semantic vectors\n",
    "print(\"Building semantic vectors with Random Indexing...\")\n",
    "semantic_vectors = build_semantic_vectors(sentences, window_size=3)\n",
    "\n",
    "print(f\"\\nBuilt semantic vectors for {len(semantic_vectors)} words\")\n",
    "print(f\"Each vector accumulated from context co-occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Semantic Similarity - Finding Related Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_words(word: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "    \"\"\"Find most similar words to a given word.\"\"\"\n",
    "    if word not in semantic_vectors:\n",
    "        return []\n",
    "    \n",
    "    word_vec = semantic_vectors[word]\n",
    "    \n",
    "    # Compute similarity to all other words\n",
    "    similarities = []\n",
    "    for other_word, other_vec in semantic_vectors.items():\n",
    "        if other_word != word:\n",
    "            sim = cosine_similarity(word_vec, other_vec)\n",
    "            similarities.append((other_word, float(sim)))\n",
    "    \n",
    "    # Sort by similarity\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Test semantic similarity\n",
    "test_words = [\"king\", \"queen\", \"france\", \"paris\", \"doctor\", \"man\", \"woman\"]\n",
    "\n",
    "print(\"Semantic Similarity - Most Related Words:\\n\")\n",
    "for word in test_words:\n",
    "    if word in semantic_vectors:\n",
    "        similar = find_similar_words(word, top_k=5)\n",
    "        print(f\"{word:12s} -> {', '.join([f'{w}({s:.2f})' for w, s in similar[:3]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Word Analogies - The Famous Examples\n",
    "\n",
    "Word analogies use vector arithmetic:\n",
    "\n",
    "**\"king is to queen as man is to woman\"**\n",
    "```\n",
    "king - man + woman ≈ queen\n",
    "```\n",
    "\n",
    "**\"Paris is to France as London is to England\"**\n",
    "```\n",
    "Paris - France + England ≈ London\n",
    "```\n",
    "\n",
    "This works because:\n",
    "- `king - man` captures \"royalty + male\"\n",
    "- Adding `woman` gives \"royalty + female\" ≈ queen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_analogy(a: str, b: str, c: str, top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "    \"\"\"Solve analogy: a is to b as c is to ?\n",
    "    \n",
    "    Computes: a - b + c ≈ ?\n",
    "    \"\"\"\n",
    "    if a not in semantic_vectors or b not in semantic_vectors or c not in semantic_vectors:\n",
    "        return []\n",
    "    \n",
    "    # Vector arithmetic: a - b + c\n",
    "    result_vec = semantic_vectors[a] - semantic_vectors[b] + semantic_vectors[c]\n",
    "    \n",
    "    # Find most similar words\n",
    "    similarities = []\n",
    "    for word, vec in semantic_vectors.items():\n",
    "        # Exclude input words\n",
    "        if word not in [a, b, c]:\n",
    "            sim = cosine_similarity(result_vec, vec)\n",
    "            similarities.append((word, float(sim)))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Test word analogies\n",
    "analogies = [\n",
    "    (\"king\", \"man\", \"woman\"),       # king - man + woman = queen\n",
    "    (\"king\", \"queen\", \"prince\"),    # king - queen + prince = ?\n",
    "    (\"paris\", \"france\", \"england\"),  # Paris - France + England = London\n",
    "    (\"paris\", \"france\", \"germany\"),  # Paris - France + Germany = Berlin\n",
    "    (\"man\", \"king\", \"woman\"),       # man - king + woman = ?\n",
    "    (\"father\", \"man\", \"woman\"),     # father - man + woman = mother\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"WORD ANALOGIES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for a, b, c in analogies:\n",
    "    results = word_analogy(a, b, c, top_k=3)\n",
    "    if results:\n",
    "        top_answer = results[0]\n",
    "        print(f\"\\n{a:10s} - {b:10s} + {c:10s} = {top_answer[0]:10s} (confidence: {top_answer[1]:.3f})\")\n",
    "        print(f\"  Other candidates: {', '.join([f'{w}({s:.3f})' for w, s in results[1:3]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Analyzing the Results\n",
    "\n",
    "Let's analyze some specific analogies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DETAILED ANALOGY ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Gender analogy\n",
    "print(\"\\n1. Gender Analogy: king - man + woman = ?\")\n",
    "results = word_analogy(\"king\", \"man\", \"woman\", top_k=10)\n",
    "print(f\"\\nTop 10 results:\")\n",
    "for i, (word, score) in enumerate(results, 1):\n",
    "    marker = \"✓\" if word == \"queen\" else \" \"\n",
    "    print(f\"{marker} {i:2d}. {word:15s} {score:.4f}\")\n",
    "\n",
    "# Capital city analogy\n",
    "print(\"\\n2. Capital City Analogy: paris - france + england = ?\")\n",
    "results = word_analogy(\"paris\", \"france\", \"england\", top_k=10)\n",
    "print(f\"\\nTop 10 results:\")\n",
    "for i, (word, score) in enumerate(results, 1):\n",
    "    marker = \"✓\" if word == \"london\" else \" \"\n",
    "    print(f\"{marker} {i:2d}. {word:15s} {score:.4f}\")\n",
    "\n",
    "# Family analogy\n",
    "print(\"\\n3. Family Analogy: father - man + woman = ?\")\n",
    "results = word_analogy(\"father\", \"man\", \"woman\", top_k=10)\n",
    "print(f\"\\nTop 10 results:\")\n",
    "for i, (word, score) in enumerate(results, 1):\n",
    "    marker = \"✓\" if word == \"mother\" else \" \"\n",
    "    print(f\"{marker} {i:2d}. {word:15s} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Comparing VSA Models\n",
    "\n",
    "Let's compare FHRR, MAP, and Binary models for word analogies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_on_analogies(model_name: str, model, test_analogies: List[Tuple[str, str, str, str]]):\n",
    "    \"\"\"Test a VSA model on word analogies.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model\n",
    "        model: VSAModel instance\n",
    "        test_analogies: List of (a, b, c, expected) tuples\n",
    "    \"\"\"\n",
    "    memory = VSAMemory(model)\n",
    "    memory.add_many(list(vocabulary))\n",
    "    \n",
    "    # Build semantic vectors\n",
    "    context_vectors = defaultdict(lambda: jnp.zeros(model.dim, \n",
    "                                                     dtype=jnp.complex64 if model_name == \"FHRR\" else jnp.float32))\n",
    "    \n",
    "    for sent in sentences:\n",
    "        for i, word in enumerate(sent):\n",
    "            start = max(0, i - 3)\n",
    "            end = min(len(sent), i + 4)\n",
    "            for j in range(start, end):\n",
    "                if j != i:\n",
    "                    context_vectors[word] = context_vectors[word] + memory[sent[j]].vec\n",
    "    \n",
    "    # Test analogies\n",
    "    correct = 0\n",
    "    results = []\n",
    "    \n",
    "    for a, b, c, expected in test_analogies:\n",
    "        if all(w in context_vectors for w in [a, b, c, expected]):\n",
    "            result_vec = context_vectors[a] - context_vectors[b] + context_vectors[c]\n",
    "            \n",
    "            # Find best match\n",
    "            best_word = None\n",
    "            best_sim = -float('inf')\n",
    "            \n",
    "            for word, vec in context_vectors.items():\n",
    "                if word not in [a, b, c]:\n",
    "                    sim = float(cosine_similarity(result_vec, vec))\n",
    "                    if sim > best_sim:\n",
    "                        best_sim = sim\n",
    "                        best_word = word\n",
    "            \n",
    "            is_correct = (best_word == expected)\n",
    "            if is_correct:\n",
    "                correct += 1\n",
    "            \n",
    "            results.append((f\"{a}-{b}+{c}\", expected, best_word, best_sim, is_correct))\n",
    "    \n",
    "    accuracy = correct / len(results) if results else 0\n",
    "    return accuracy, results\n",
    "\n",
    "# Test analogies (a, b, c, expected)\n",
    "test_analogies = [\n",
    "    (\"king\", \"man\", \"woman\", \"queen\"),\n",
    "    (\"paris\", \"france\", \"england\", \"london\"),\n",
    "    (\"paris\", \"france\", \"germany\", \"berlin\"),\n",
    "    (\"father\", \"man\", \"woman\", \"mother\"),\n",
    "]\n",
    "\n",
    "# Test models\n",
    "models_to_test = [\n",
    "    (\"FHRR\", create_fhrr_model(dim=512)),\n",
    "    (\"MAP\", create_map_model(dim=512)),\n",
    "    (\"Binary\", create_binary_model(dim=10000, bipolar=True)),\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL COMPARISON ON WORD ANALOGIES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name, model in models_to_test:\n",
    "    print(f\"\\nTesting {model_name} model (dim={model.dim})...\")\n",
    "    accuracy, results = test_model_on_analogies(model_name, model, test_analogies)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.1%} ({int(accuracy * len(results))}/{len(results)} correct)\\n\")\n",
    "    \n",
    "    for query, expected, predicted, confidence, correct in results:\n",
    "        marker = \"✓\" if correct else \"✗\"\n",
    "        print(f\"  {marker} {query:25s} -> {predicted:10s} (expected: {expected}, conf: {confidence:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Understanding What Makes This Work\n",
    "\n",
    "Why do word analogies work with VSA?\n",
    "\n",
    "1. **Distributional Semantics**: Words with similar contexts have similar meanings\n",
    "2. **Vector Arithmetic**: Differences capture relationships\n",
    "3. **High-Dimensional Geometry**: Many relationships can coexist without interference\n",
    "\n",
    "**Example**: `king - man`\n",
    "- `king` vector contains: {royalty, male, power, leadership, ...}\n",
    "- `man` vector contains: {male, adult, ...}\n",
    "- `king - man` ≈ {royalty, power, leadership} (removes maleness)\n",
    "- Adding `woman` gives {royalty, power, leadership, female} ≈ queen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze vector compositions\n",
    "print(\"=\" * 70)\n",
    "print(\"VECTOR COMPOSITION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if all(w in semantic_vectors for w in [\"king\", \"man\", \"woman\", \"queen\"]):\n",
    "    king = semantic_vectors[\"king\"]\n",
    "    man = semantic_vectors[\"man\"]\n",
    "    woman = semantic_vectors[\"woman\"]\n",
    "    queen = semantic_vectors[\"queen\"]\n",
    "    \n",
    "    # Compute relationships\n",
    "    king_minus_man = king - man\n",
    "    queen_minus_woman = queen - woman\n",
    "    king_minus_queen = king - queen\n",
    "    man_minus_woman = man - woman\n",
    "    \n",
    "    print(\"\\n1. Gender-neutral royalty (king - man vs queen - woman):\")\n",
    "    sim = cosine_similarity(king_minus_man, queen_minus_woman)\n",
    "    print(f\"   Similarity: {sim:.4f}\")\n",
    "    print(f\"   Interpretation: Both capture 'royalty' concept\")\n",
    "    \n",
    "    print(\"\\n2. Royalty difference (king - queen):\")\n",
    "    print(f\"   This should be similar to (man - woman)\")\n",
    "    sim = cosine_similarity(king_minus_queen, man_minus_woman)\n",
    "    print(f\"   Similarity: {sim:.4f}\")\n",
    "    print(f\"   Interpretation: Both capture gender difference\")\n",
    "    \n",
    "    print(\"\\n3. The analogy:\")\n",
    "    result = king - man + woman\n",
    "    sim_to_queen = cosine_similarity(result, queen)\n",
    "    print(f\"   king - man + woman ~ queen\")\n",
    "    print(f\"   Similarity to 'queen': {sim_to_queen:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Random Indexing builds semantic vectors** from co-occurrence patterns\n",
    "2. **Context shapes meaning**: Words in similar contexts have similar vectors\n",
    "3. **Vector arithmetic enables analogies**: Differences capture relationships\n",
    "4. **High dimensions are crucial**: Allows many relationships to coexist\n",
    "5. **Model choice matters**: FHRR provides best semantic similarity for this task\n",
    "\n",
    "## Limitations & Extensions\n",
    "\n",
    "**Current Limitations**:\n",
    "- Small corpus (limited vocabulary and relationships)\n",
    "- Simple window-based context (no weighting by distance)\n",
    "- No frequency weighting (common words vs rare words)\n",
    "\n",
    "**Possible Extensions**:\n",
    "1. **Larger corpus**: Wikipedia, books, news articles\n",
    "2. **Weighted context**: Words closer to target weighted more\n",
    "3. **Stop word filtering**: Remove \"the\", \"a\", \"is\", etc.\n",
    "4. **Frequency weighting**: Rare words more informative\n",
    "5. **Multiple passes**: Iterate to refine vectors\n",
    "6. **Visualization**: PCA/t-SNE to plot word space\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try larger corpora (download from nltk or huggingface)\n",
    "- Implement stop word filtering\n",
    "- Add distance weighting in context window\n",
    "- Compare with modern embeddings (Word2Vec, GloVe)\n",
    "- Explore other analogy types (verb tenses, plurals, comparatives)\n",
    "\n",
    "## References\n",
    "\n",
    "- Kanerva, P., Kristoferson, J., & Holst, A. (2000). \"Random Indexing of text samples for Latent Semantic Analysis\"\n",
    "- Landauer, T., & Dumais, S. (1997). \"A solution to Plato's problem: The Latent Semantic Analysis theory\"\n",
    "- Mikolov et al. (2013). \"Efficient Estimation of Word Representations in Vector Space\" (Word2Vec)\n",
    "\n",
    "## Running This Tutorial\n",
    "\n",
    "```bash\n",
    "jupyter notebook examples/notebooks/tutorial_04_word_analogies.ipynb\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
