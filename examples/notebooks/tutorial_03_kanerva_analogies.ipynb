{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Analogical Reasoning - Kanerva's \"Dollar of Mexico\"\n",
    "\n",
    "This tutorial implements the classic examples from Pentti Kanerva's 2010 paper:\n",
    "**\"What We Mean When We Say 'What's the Dollar of Mexico?': Prototypes and Mapping in Concept Space\"**\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Encode structured records holistically (countries with name, capital, currency)\n",
    "- Compute mapping vectors from examples\n",
    "- Perform analogical queries (\"What's the dollar of Mexico?\")\n",
    "- Solve IQ-test style analogies\n",
    "- Chain mappings for transitive reasoning\n",
    "- Compare Binary and FHRR models for analogy\n",
    "\n",
    "## Why Analogical Reasoning?\n",
    "\n",
    "From the paper:\n",
    "> \"Figurative language is pervasive, bypasses the literal meaning of what is said and is interpreted metaphorically or by analogy.\"\n",
    "\n",
    "When we say \"the peso is the Mexican dollar,\" we're using analogy:\n",
    "- We map concepts from one domain (US) to another (Mexico)\n",
    "- The mapping preserves structure and relationships\n",
    "- VSA makes such mappings **computable** through simple operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from vsax import create_binary_model, create_fhrr_model\n",
    "from vsax import VSAMemory\n",
    "from vsax.similarity import cosine_similarity, hamming_similarity\n",
    "\n",
    "# Use Binary model (as in Kanerva's paper)\n",
    "# Binary uses XOR for binding and majority vote for bundling\n",
    "model = create_binary_model(dim=10000, bipolar=True)\n",
    "memory = VSAMemory(model)\n",
    "\n",
    "print(f\"Model: {model.rep_cls.__name__}\")\n",
    "print(f\"Dimension: {model.dim}\")\n",
    "print(f\"Binding: XOR (self-inverse)\")\n",
    "print(f\"Bundling: Majority vote\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Encoding Holistic Records\n",
    "\n",
    "Following Kanerva's paper, we encode countries as structured records with three attributes:\n",
    "- **NAM**: Name of the country\n",
    "- **CAP**: Capital city\n",
    "- **MON**: Monetary unit\n",
    "\n",
    "A country is encoded as:\n",
    "```\n",
    "COUNTRY = [(NAM * name) + (CAP * capital) + (MON * currency)]\n",
    "```\n",
    "\n",
    "where `*` is binding (XOR) and `+` is bundling (majority vote)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create basis vectors for attributes (roles)\n",
    "memory.add_many([\"NAM\", \"CAP\", \"MON\"])\n",
    "\n",
    "# Create basis vectors for values (fillers)\n",
    "countries_data = {\n",
    "    \"United States\": {\"name\": \"USA\", \"capital\": \"WDC\", \"currency\": \"DOL\"},\n",
    "    \"Mexico\": {\"name\": \"MEX\", \"capital\": \"MXC\", \"currency\": \"PES\"},\n",
    "    \"Sweden\": {\"name\": \"SWE\", \"capital\": \"STO\", \"currency\": \"KRO\"},\n",
    "    \"Japan\": {\"name\": \"JPN\", \"capital\": \"TOK\", \"currency\": \"YEN\"},\n",
    "    \"France\": {\"name\": \"FRA\", \"capital\": \"PAR\", \"currency\": \"EUR\"},\n",
    "}\n",
    "\n",
    "# Add all fillers to memory\n",
    "all_fillers = []\n",
    "for data in countries_data.values():\n",
    "    all_fillers.extend(data.values())\n",
    "memory.add_many(all_fillers)\n",
    "\n",
    "print(f\"Created {len(memory)} basis vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_country(name: str, capital: str, currency: str):\n",
    "    \"\"\"Encode a country as a holistic vector.\n",
    "    \n",
    "    COUNTRY = [(NAM * name) + (CAP * capital) + (MON * currency)]\n",
    "    \"\"\"\n",
    "    nam_hv = memory[\"NAM\"]\n",
    "    cap_hv = memory[\"CAP\"]\n",
    "    mon_hv = memory[\"MON\"]\n",
    "    \n",
    "    name_hv = memory[name]\n",
    "    capital_hv = memory[capital]\n",
    "    currency_hv = memory[currency]\n",
    "    \n",
    "    # Bind each role with its filler\n",
    "    nam_bound = model.opset.bind(nam_hv.vec, name_hv.vec)\n",
    "    cap_bound = model.opset.bind(cap_hv.vec, capital_hv.vec)\n",
    "    mon_bound = model.opset.bind(mon_hv.vec, currency_hv.vec)\n",
    "    \n",
    "    # Bundle all role-filler pairs\n",
    "    country_vec = model.opset.bundle(nam_bound, cap_bound, mon_bound)\n",
    "    \n",
    "    return model.rep_cls(country_vec)\n",
    "\n",
    "# Encode countries\n",
    "USTATES = encode_country(\"USA\", \"WDC\", \"DOL\")\n",
    "MEXICO = encode_country(\"MEX\", \"MXC\", \"PES\")\n",
    "SWEDEN = encode_country(\"SWE\", \"STO\", \"KRO\")\n",
    "JAPAN = encode_country(\"JPN\", \"TOK\", \"YEN\")\n",
    "FRANCE = encode_country(\"FRA\", \"PAR\", \"EUR\")\n",
    "\n",
    "print(\"Encoded countries as holistic vectors\")\n",
    "print(f\"USTATES shape: {USTATES.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying Holistic Records\n",
    "\n",
    "We can extract values from the holistic encoding:\n",
    "```\n",
    "MON * USTATES ≈ DOL\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_attribute(country_hv, attribute: str) -> str:\n",
    "    \"\"\"Query an attribute from a country vector.\"\"\"\n",
    "    attr_hv = memory[attribute]\n",
    "    \n",
    "    # Unbind: attribute * country ≈ value\n",
    "    result = model.opset.bind(attr_hv.vec, country_hv.vec)\n",
    "    \n",
    "    # Find most similar filler\n",
    "    best_match = None\n",
    "    best_sim = -1\n",
    "    \n",
    "    for filler in all_fillers:\n",
    "        sim = hamming_similarity(result, memory[filler].vec)\n",
    "        if sim > best_sim:\n",
    "            best_sim = sim\n",
    "            best_match = filler\n",
    "    \n",
    "    return best_match, best_sim\n",
    "\n",
    "# Test queries\n",
    "print(\"Querying holistic country vectors:\\n\")\n",
    "for country_name, country_hv in [(\"USA\", USTATES), (\"Mexico\", MEXICO), (\"Sweden\", SWEDEN)]:\n",
    "    name, sim = query_attribute(country_hv, \"NAM\")\n",
    "    capital, _ = query_attribute(country_hv, \"CAP\")\n",
    "    currency, _ = query_attribute(country_hv, \"MON\")\n",
    "    print(f\"{country_name:10s} -> name={name}, capital={capital}, currency={currency}, sim={sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Computing Mapping Vectors from Examples\n",
    "\n",
    "The key insight from Kanerva's paper:\n",
    "\n",
    "**A mapping vector can be computed from a single example pair!**\n",
    "\n",
    "```\n",
    "F_UM = USTATES * MEXICO\n",
    "```\n",
    "\n",
    "This vector `F_UM` encodes the mapping from US to Mexico:\n",
    "```\n",
    "F_UM = [(USA * MEX) + (WDC * MXC) + (DOL * PES) + noise]\n",
    "```\n",
    "\n",
    "The structure (roles) cancels out, leaving only the **prototype-based** mapping!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mapping from US to Mexico\n",
    "F_UM = model.opset.bind(USTATES.vec, MEXICO.vec)\n",
    "F_UM_hv = model.rep_cls(F_UM)\n",
    "\n",
    "print(\"Computed mapping vector F_UM = USTATES * MEXICO\")\n",
    "print(f\"This vector maps concepts from US domain to Mexico domain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The Famous \"Dollar of Mexico\" Query\n",
    "\n",
    "Now we can answer: **\"What's the dollar of Mexico?\"**\n",
    "\n",
    "```\n",
    "DOL * F_UM ≈ PES\n",
    "```\n",
    "\n",
    "The mapping vector transforms \"dollar\" into its Mexican equivalent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_concept(concept: str, mapping_vec) -> str:\n",
    "    \"\"\"Map a concept using a mapping vector.\"\"\"\n",
    "    concept_hv = memory[concept]\n",
    "    \n",
    "    # Apply mapping: concept * F ≈ mapped_concept\n",
    "    result = model.opset.bind(concept_hv.vec, mapping_vec)\n",
    "    \n",
    "    # Find most similar concept\n",
    "    best_match = None\n",
    "    best_sim = -1\n",
    "    \n",
    "    for filler in all_fillers:\n",
    "        sim = hamming_similarity(result, memory[filler].vec)\n",
    "        if sim > best_sim:\n",
    "            best_sim = sim\n",
    "            best_match = filler\n",
    "    \n",
    "    return best_match, best_sim\n",
    "\n",
    "# The famous query!\n",
    "print(\"=\" * 60)\n",
    "print(\"What's the Dollar of Mexico?\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result, confidence = map_concept(\"DOL\", F_UM)\n",
    "print(f\"\\nDOL * F_UM = {result} (confidence: {confidence:.3f})\")\n",
    "print(f\"\\nAnswer: The peso is the Mexican dollar!\")\n",
    "\n",
    "# Try other mappings\n",
    "print(\"\\nOther US -> Mexico mappings:\")\n",
    "for concept in [\"USA\", \"WDC\"]:\n",
    "    result, conf = map_concept(concept, F_UM)\n",
    "    print(f\"  {concept} -> {result} (confidence: {conf:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: IQ Test Analogy\n",
    "\n",
    "From the paper:\n",
    "```\n",
    "United States : Mexico :: Dollar : ?\n",
    "```\n",
    "\n",
    "We know:\n",
    "```\n",
    "Peso : Mexico :: Dollar : United States\n",
    "```\n",
    "\n",
    "Some function F maps both pairs:\n",
    "```\n",
    "F * DOL = USTATES\n",
    "F * PES = MEXICO\n",
    "```\n",
    "\n",
    "Solving for F:\n",
    "```\n",
    "USTATES * DOL = MEXICO * PES\n",
    "```\n",
    "\n",
    "Therefore:\n",
    "```\n",
    "PES = MEXICO * USTATES * DOL\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"IQ Test: United States : Mexico :: Dollar : ?\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compute the answer\n",
    "# PES = MEXICO * USTATES * DOL\n",
    "mapping = model.opset.bind(MEXICO.vec, USTATES.vec)\n",
    "answer_vec = model.opset.bind(mapping, memory[\"DOL\"].vec)\n",
    "\n",
    "# Find best match\n",
    "best_match = None\n",
    "best_sim = -1\n",
    "for filler in all_fillers:\n",
    "    sim = hamming_similarity(answer_vec, memory[filler].vec)\n",
    "    if sim > best_sim:\n",
    "        best_sim = sim\n",
    "        best_match = filler\n",
    "\n",
    "print(f\"\\nMEXICO * USTATES * DOL = {best_match} (confidence: {best_sim:.3f})\")\n",
    "print(f\"\\nAnswer: Peso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Transitive Mappings\n",
    "\n",
    "From the paper:\n",
    "```\n",
    "F_SU = SWEDEN * USTATES    (Sweden -> US)\n",
    "F_UM = USTATES * MEXICO     (US -> Mexico)\n",
    "F_SM = F_SU * F_UM          (Sweden -> Mexico)\n",
    "     = SWEDEN * MEXICO\n",
    "```\n",
    "\n",
    "Mappings can be **chained** like translating through multiple languages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Transitive Mapping: Sweden -> US -> Mexico\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compute individual mappings\n",
    "F_SU = model.opset.bind(SWEDEN.vec, USTATES.vec)  # Sweden -> US\n",
    "F_UM = model.opset.bind(USTATES.vec, MEXICO.vec)  # US -> Mexico\n",
    "\n",
    "# Chain them\n",
    "F_SM_chained = model.opset.bind(F_SU, F_UM)\n",
    "\n",
    "# Direct mapping\n",
    "F_SM_direct = model.opset.bind(SWEDEN.vec, MEXICO.vec)\n",
    "\n",
    "# They should be the same!\n",
    "similarity = hamming_similarity(F_SM_chained, F_SM_direct)\n",
    "print(f\"\\nF_SU * F_UM ≈ SWEDEN * MEXICO\")\n",
    "print(f\"Similarity: {similarity:.3f}\")\n",
    "\n",
    "# Test the chained mapping\n",
    "print(\"\\nUsing chained mapping (Sweden -> US -> Mexico):\")\n",
    "for concept in [\"SWE\", \"STO\", \"KRO\"]:\n",
    "    result, conf = map_concept(concept, F_SM_chained)\n",
    "    print(f\"  {concept} -> {result} (confidence: {conf:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Multiple Countries - Learning the Pattern\n",
    "\n",
    "Let's verify the mapping works for all countries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_vectors = {\n",
    "    \"USA\": USTATES,\n",
    "    \"Mexico\": MEXICO,\n",
    "    \"Sweden\": SWEDEN,\n",
    "    \"Japan\": JAPAN,\n",
    "    \"France\": FRANCE,\n",
    "}\n",
    "\n",
    "country_currencies = {\n",
    "    \"USA\": \"DOL\",\n",
    "    \"Mexico\": \"PES\",\n",
    "    \"Sweden\": \"KRO\",\n",
    "    \"Japan\": \"YEN\",\n",
    "    \"France\": \"EUR\",\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"What's the dollar of X?\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for target_country in [\"Mexico\", \"Sweden\", \"Japan\", \"France\"]:\n",
    "    # Compute mapping US -> target\n",
    "    mapping = model.opset.bind(USTATES.vec, country_vectors[target_country].vec)\n",
    "    \n",
    "    # Map dollar\n",
    "    result, conf = map_concept(\"DOL\", mapping)\n",
    "    expected = country_currencies[target_country]\n",
    "    \n",
    "    match = \"✓\" if result == expected else \"✗\"\n",
    "    print(f\"{match} The dollar of {target_country:10s} is {result} (expected: {expected}, conf: {conf:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Comparing Binary vs FHRR Models\n",
    "\n",
    "Kanerva's paper uses Binary (XOR) for simplicity. Let's compare with FHRR (complex vectors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_analogy_model(model_name: str, model):\n",
    "    \"\"\"Test analogical reasoning with a given model.\"\"\"\n",
    "    memory = VSAMemory(model)\n",
    "    \n",
    "    # Add concepts\n",
    "    memory.add_many([\"NAM\", \"CAP\", \"MON\"] + all_fillers)\n",
    "    \n",
    "    # Encode countries\n",
    "    def encode(name, cap, curr):\n",
    "        nam_bound = model.opset.bind(memory[\"NAM\"].vec, memory[name].vec)\n",
    "        cap_bound = model.opset.bind(memory[\"CAP\"].vec, memory[cap].vec)\n",
    "        mon_bound = model.opset.bind(memory[\"MON\"].vec, memory[curr].vec)\n",
    "        return model.rep_cls(model.opset.bundle(nam_bound, cap_bound, mon_bound))\n",
    "    \n",
    "    us = encode(\"USA\", \"WDC\", \"DOL\")\n",
    "    mx = encode(\"MEX\", \"MXC\", \"PES\")\n",
    "    \n",
    "    # Compute mapping\n",
    "    f_um = model.opset.bind(us.vec, mx.vec)\n",
    "    \n",
    "    # Map dollar to peso\n",
    "    result = model.opset.bind(memory[\"DOL\"].vec, f_um)\n",
    "    \n",
    "    # Measure similarity to peso\n",
    "    similarity = cosine_similarity(result, memory[\"PES\"].vec)\n",
    "    \n",
    "    return float(similarity)\n",
    "\n",
    "# Test both models\n",
    "binary_model = create_binary_model(dim=10000, bipolar=True)\n",
    "fhrr_model = create_fhrr_model(dim=512)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Model Comparison: Dollar -> Peso Mapping\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "binary_sim = test_analogy_model(\"Binary\", binary_model)\n",
    "fhrr_sim = test_analogy_model(\"FHRR\", fhrr_model)\n",
    "\n",
    "print(f\"\\nBinary (XOR, dim=10000):   {binary_sim:.4f}\")\n",
    "print(f\"FHRR (Complex, dim=512):    {fhrr_sim:.4f}\")\n",
    "print(f\"\\nBoth models successfully learn analogical mappings!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "From Kanerva's paper, we've learned:\n",
    "\n",
    "1. **Holistic Encoding**: Structure can be encoded without explicit fields\n",
    "2. **Mapping as First-Class Operation**: `F = A * B` creates a mapping\n",
    "3. **Distance Preservation**: Mappings preserve relationships\n",
    "4. **Prototypes vs Variables**: Concrete examples (prototypes) replace abstract variables\n",
    "5. **Composable Mappings**: Mappings can be chained transitively\n",
    "6. **Learning from Examples**: A single example pair defines a mapping\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "From the paper:\n",
    "> \"The readily available mapping operations could determine the kinds of concept spaces we can build and make use of. The emergence of such mapping functions could have led to the development of human language.\"\n",
    "\n",
    "VSA provides a **computational model** for:\n",
    "- Analogical reasoning\n",
    "- Metaphorical language\n",
    "- Transfer learning\n",
    "- Abstract thought\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try more complex analogies\n",
    "- Explore analogies in other domains (geometric shapes, word relationships)\n",
    "- Combine with knowledge graphs for richer reasoning\n",
    "- Investigate noise tolerance and dimensionality trade-offs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
