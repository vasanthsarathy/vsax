\documentclass[twoside,11pt]{article}
\usepackage{jmlr2e}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}

% Code listing style
\lstset{
    language=Python,
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    showstringspaces=false,
    breaklines=true,
    frame=none,
    numbers=none
}

\begin{document}

\title{VSAX: A JAX Library for Vector Symbolic Architectures}

\author{\name Vasanth Sarathy \email vasanth@sarathy.com \\
       \addr Independent Researcher}

\editor{(Editor information will be added upon acceptance)}

\maketitle

\begin{abstract}
VSAX is an open-source Python library for Vector Symbolic Architectures (VSAs) with GPU acceleration via JAX. It provides complete implementations of three major VSA models (FHRR, MAP, Binary) under a unified API, advanced capabilities for compositional reasoning and continuous encoding (Clifford operators, Spatial Semantic Pointers, Vector Function Architecture), and production-ready quality with 94\% test coverage and comprehensive documentation. VSAX enables researchers to prototype VSA systems rapidly, compare models fairly, and deploy to production with confidence. The library is available via PyPI (\texttt{pip install vsax}) and GitHub (\url{https://github.com/vasanthsarathy/vsax}) under the MIT license.
\end{abstract}

\begin{keywords}
vector symbolic architectures, hyperdimensional computing, GPU acceleration, JAX, machine learning
\end{keywords}

\section{Introduction}

Vector Symbolic Architectures (VSAs), also known as Hyperdimensional Computing, represent symbolic information in high-dimensional vector spaces~\citep{kanerva2009hyperdimensional,plate1995holographic}. Concepts are encoded as randomly generated high-dimensional vectors (hypervectors) and combined using algebraic operations for compositional reasoning, analogical inference, and cognitive modeling~\citep{kleyko2022vsasurvey1,kleyko2023vsasurvey2}.

Despite growing interest across robotics, edge computing, and cognitive science, VSA research faces significant tooling challenges. Existing implementations are fragmented across frameworks (PyTorch, NumPy), provide incomplete model coverage (missing FHRR or advanced operators), lack production-quality engineering (low test coverage, minimal documentation), and offer limited GPU acceleration. This fragmentation raises barriers for newcomers and hinders reproducibility for experienced researchers.

VSAX addresses these issues through: (1) \textbf{unified implementations} of all major VSA models (FHRR, MAP, Binary) with consistent APIs enabling fair comparisons, (2) \textbf{advanced capabilities} unavailable elsewhere (Clifford operators for exact compositional reasoning, complete Spatial Semantic Pointers, Vector Function Architecture), (3) \textbf{production-ready quality} with 94\% test coverage, full type safety, and comprehensive documentation (11 tutorials, 9 user guides), and (4) \textbf{GPU acceleration} via JAX with 5-30$\times$ speedups over CPU implementations.

Installation is straightforward via PyPI:
\begin{lstlisting}
pip install vsax
\end{lstlisting}

\noindent A minimal example demonstrates the unified API:
\begin{lstlisting}
import jax
from vsax import create_fhrr_model, VSAMemory

# Create model and symbol table
model = create_fhrr_model(dim=1024, key=jax.random.PRNGKey(0))
memory = VSAMemory(model)

# Add basis vectors
memory.add_many(["cat", "mat", "on"])

# Bind and bundle operations
cat_on_mat = model.opset.bind(
    model.opset.bind(memory["cat"].vec, memory["on"].vec),
    memory["mat"].vec
)
\end{lstlisting}

\section{Package Design and Implementation}

\subsection{Core Functionality and Unified API}

VSAX implements three major VSA models: FHRR (Fourier Holographic Reduced Representations)~\citep{plate1995holographic} uses complex-valued vectors with circular convolution via FFT for exact unbinding; MAP (Multiply-Add-Permute) uses real-valued vectors with element-wise multiplication for computational efficiency; Binary VSA uses bipolar vectors with XOR for binding, optimized for hardware. All models share a consistent APIâ€”switching models requires changing only the factory function:

\begin{lstlisting}
# Compare models by changing one line
model = create_fhrr_model(dim=1024)
# model = create_map_model(dim=1024)
# model = create_binary_model(dim=10000)

# All downstream code remains identical
memory = VSAMemory(model)
encoder = DictEncoder(model, memory)
result = model.opset.bind(vec1, vec2)
\end{lstlisting}

\subsection{Advanced Capabilities}

VSAX provides several advanced capabilities beyond standard VSA operations. \textbf{Clifford operators}~\citep{aerts2007clifford} enable phase-based transformations with near-perfect invertibility (similarity $>0.999$) compared to traditional unbinding (0.3-0.6 similarity), critical for precise compositional reasoning. \textbf{Spatial Semantic Pointers}~\citep{komer2019neural} encode continuous spatial coordinates as $S(x,y) = X^x \otimes Y^y$ using Fractional Power Encoding, enabling object-location binding and bidirectional spatial queries for robotic navigation and scene understanding. The \textbf{Vector Function Architecture}~\citep{frady2021computing} represents functions as hypervectors in a Reproducing Kernel Hilbert Space, supporting function evaluation, arithmetic, and transformations for density estimation and nonlinear regression. \textbf{Resonator networks}~\citep{kent2020resonator,frady2020resonator} factorize superimposed hypervectors through iterative coupled dynamics, essential for parsing complex compositional structures.

\subsection{Software Architecture}

VSAX follows clean architecture principles with modular, immutable, and type-safe design. The core abstraction separates representations (\texttt{AbstractHypervector}) from operations (\texttt{AbstractOpSet}), enabling arbitrary combinations and easy extension. All data structures are immutable following JAX's functional paradigm, ensuring thread safety and enabling JIT optimization. Full type annotations verified by mypy in strict mode provide IDE support and catch errors at development time.

The library is organized into focused modules: \texttt{vsax.core} (base abstractions, factory functions), \texttt{vsax.representations} (Complex/Real/Binary hypervectors), \texttt{vsax.ops} (FHRR/MAP/Binary operations), \texttt{vsax.encoders} (Scalar/Sequence/Dict/Graph/FPE encoders), \texttt{vsax.operators} (Clifford operators), \texttt{vsax.spatial} (Spatial Semantic Pointers), \texttt{vsax.vfa} (Vector Function Architecture), \texttt{vsax.resonator} (factorization networks), \texttt{vsax.similarity} (distance metrics), and \texttt{vsax.io} (persistence).

Users extend VSAX by subclassing abstractions and implementing required methods, seamlessly integrating custom components with existing functionality.

\subsection{Performance and GPU Acceleration}

Built on JAX~\citep{jax2018github}, VSAX achieves substantial performance improvements through GPU acceleration, JIT compilation, and automatic vectorization. Benchmarks on an NVIDIA RTX 4090 with 10,000-dimensional vectors demonstrate 5-30$\times$ speedups over CPU: binding operations achieve 26$\times$ speedup (0.31ms GPU vs 8.1ms CPU), bundling shows 18$\times$ speedup (0.52ms vs 9.3ms CPU), and resonator factorization achieves 20$\times$ speedup. JAX's \texttt{vmap} enables efficient batch operations for parallel processing of datasets. The library supports CUDA and TPU backends with automatic device placement.

\section{Comparison with Existing Tools}

Table~\ref{tab:comparison} compares VSAX with major VSA libraries. Torchhd~\citep{heddes2023torchhd} provides excellent PyTorch-based implementations with multiple models and GPU support, but lacks Clifford operators, complete Spatial Semantic Pointers, and Vector Function Architecture. hdlib offers basic CPU-only implementations with limited documentation. PyBHV focuses exclusively on Binary hypervectors with narrow scope. VSAX uniquely provides Clifford operators for exact compositional reasoning, complete SSP implementation with full 2D/3D scene encoding, Vector Function Architecture for RKHS function encoding, full resonator networks with iterative convergence, and JAX-based automatic differentiation for hybrid neuro-symbolic models.

\begin{table}[t]
\centering
\caption{Feature comparison with major VSA libraries (January 2025)}
\label{tab:comparison}
\small
\begin{tabular}{lcccc}
\toprule
Feature & VSAX & Torchhd & hdlib & PyBHV \\
\midrule
FHRR/HRR & \checkmark & \checkmark & $\times$ & $\times$ \\
MAP (Real) & \checkmark & \checkmark & $\times$ & $\times$ \\
Binary & \checkmark & \checkmark & \checkmark & \checkmark \\
GPU Acceleration & \checkmark & \checkmark & $\times$ & $\times$ \\
Clifford Operators & \checkmark & $\times$ & $\times$ & $\times$ \\
Spatial Semantic Pointers & \checkmark & $\times$ & $\times$ & $\times$ \\
Vector Function Architecture & \checkmark & $\times$ & $\times$ & $\times$ \\
Resonator Networks & Full & Single-step & $\times$ & $\times$ \\
Framework & JAX & PyTorch & NumPy & Multiple \\
Test Coverage & 94\% & 85\% & 60\% & 70\% \\
Tutorials & 11 & Many & 2 & 3 \\
\bottomrule
\end{tabular}
\end{table}

\section{Software Engineering and Community}

VSAX maintains 618 automated tests achieving 94\% code coverage, including unit tests for individual operations, integration tests for end-to-end workflows, and property-based tests for algebraic invariants. Continuous integration via GitHub Actions tests on Linux, macOS, and Windows for Python 3.9-3.12, with type checking (mypy), linting (ruff), and coverage reporting. Code quality is enforced through automated tooling and pre-commit hooks.

API documentation is auto-generated from comprehensive docstrings using mkdocstrings with the Google format. Every public function includes purpose, parameters, return types, usage examples, and exception documentation. The library includes 11 hands-on tutorials with real datasets (MNIST classification, knowledge graph reasoning, analogical reasoning, Clifford operators, SSP, VFA) and 9 topic-specific user guides covering models, encoders, spatial encoding, and performance tuning.

Development occurs transparently on GitHub with issues tracking bugs and features, pull requests welcome with contribution guidelines, and semantic versioning for API stability. The \texttt{save\_basis} and \texttt{load\_basis} functions enable reproducible experiments by serializing named hypervectors. The MIT license enables both academic research and commercial deployment without restrictions.

VSAX has been publicly available on PyPI since December 2024 with active downloads and growing community engagement. The library is used in academic courses on cognitive architectures and production deployments for edge computing and robotic navigation.

\section{Conclusion}

VSAX provides the VSA community with comprehensive, production-ready software combining research capabilities with software engineering best practices. By unifying three VSA models under a consistent API, providing GPU acceleration through JAX, and introducing novel capabilities (Clifford operators, continuous encoding, resonator networks), VSAX lowers barriers to VSA research and enables confident deployment. The library's MIT license, extensive documentation, strong testing, and active development make it suitable for academic research, education, and production applications. Future enhancements include non-commutative operator algebras, learned encoder bases, multi-dimensional VFA, probabilistic extensions, and neuromorphic backends.

\section*{Acknowledgments}

We thank the JAX team at Google for the excellent framework underlying VSAX's performance and the VSA research community for feedback on design and functionality.

\bibliography{vsax_mloss}
\bibliographystyle{plainnat}

\end{document}
