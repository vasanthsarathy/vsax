# VSAX vs Other HDC/VSA Libraries

This document explains VSAX's design philosophy and how it compares to other open-source hyperdimensional computing libraries.

## TL;DR - When to Use VSAX

**Choose VSAX if you want:**
- âœ… JAX-native functional programming with automatic GPU acceleration
- âœ… Clean separation between representations and operations
- âœ… Composable, modular architecture for research and prototyping
- âœ… Strong theoretical grounding (implements canonical VSA models)
- âœ… Type-safe, well-documented API with 94% test coverage
- âœ… Resonator networks for factorization
- âœ… Seamless integration with JAX ecosystem (jit, vmap, grad)

**Choose alternatives if you need:**
- âŒ PyTorch integration â†’ **torchhd**
- âŒ Production ML classifiers with 150+ datasets â†’ **torchhd**
- âŒ Biomedical/medical informatics focus â†’ **hdlib**
- âŒ Advanced boolean operations and circuit compilation â†’ **PyBHV**
- âŒ Custom CUDA kernels â†’ **hdtorch**

---

## VSAX's Design Philosophy

VSAX is built on three core principles:

### 1. **JAX-Native Functional Programming**

Unlike PyTorch-based libraries (torchhd, hdtorch), VSAX is built entirely on JAX:

```python
# JAX provides automatic differentiation, JIT compilation, and vectorization
from jax import jit, vmap, grad
import jax.numpy as jnp

# VSAX operations are pure functions
result = model.opset.bind(a, b)  # Functional, composable

# Automatic GPU acceleration - no explicit device management
@jit
def fast_encoding(vectors):
    return vmap(model.opset.bundle)(vectors)
```

**Why JAX?**
- **Functional purity**: No hidden state, easier to reason about
- **Automatic transformations**: `jit`, `vmap`, `grad` work out of the box
- **Research-friendly**: Designed for ML research at Google/DeepMind
- **NumPy-like API**: Familiar interface, minimal learning curve

### 2. **Modular Architecture**

VSAX cleanly separates concerns:

```python
# Representations (data)
ComplexHypervector, RealHypervector, BinaryHypervector

# Operations (algorithms)
FHRROperations, MAPOperations, BinaryOperations

# Model (composition)
VSAModel(dim, rep_cls, opset, sampler)
```

This is different from torchhd's integrated approach where models are classes with built-in operations.

**Benefit**: Mix and match components:
- Try different operations with the same representation
- Swap representations without changing code
- Easy to add new VSA models

### 3. **Simplicity and Clarity**

VSAX prioritizes **understanding** over **features**:

- **3 canonical VSA models** (FHRR, MAP, Binary) implemented correctly
- **Clear abstractions**: Every operation has a mathematical meaning
- **Comprehensive tutorials**: Learn VSA concepts, not just API calls
- **Theory-first**: Based on foundational papers (Plate, Gayler, Kanerva)

---

## Feature Comparison

### Supported VSA Models

| Library | FHRR | MAP | Binary | HRR | Others |
|---------|------|-----|--------|-----|--------|
| **VSAX** | âœ… | âœ… | âœ… | âŒ | - |
| **torchhd** | âœ… | âœ… | âœ… (BSC) | âœ… | B-SBC, CGR, MCR, VTB |
| **hdlib** | â“ | â“ | â“ | â“ | General VSA |
| **PyBHV** | âŒ | âŒ | âœ… | âŒ | Boolean only |
| **hdtorch** | â“ | â“ | âœ… | â“ | Focus on CUDA ops |

**VSAX focuses on quality over quantity**: 3 well-implemented models vs 8+ models with varying documentation.

### Core Operations

| Feature | VSAX | torchhd | hdlib | PyBHV | hdtorch |
|---------|------|---------|-------|-------|---------|
| Binding | âœ… | âœ… | âœ… | âœ… (XOR) | âœ… |
| Bundling | âœ… | âœ… | âœ… | âœ… (Majority) | âœ… |
| Permutation | âœ… | âœ… | âœ… | âœ… | âœ… |
| Similarity | âœ… | âœ… | âœ… | âœ… | âœ… |
| Resonator Networks | âœ… | âŒ | âŒ | âŒ | âŒ |
| Memory/Cleanup | âœ… | âœ… | âœ… | âŒ | âŒ |

**VSAX unique feature**: Full implementation of resonator networks for factorization (from Frady et al. 2020).

### Encoders

| Feature | VSAX | torchhd | hdlib | PyBHV | hdtorch |
|---------|------|---------|-------|-------|---------|
| Scalar | âœ… | âœ… (Level, Thermometer) | âœ… | âŒ | âœ… |
| Sequence | âœ… | âœ… | âœ… | âŒ | âœ… |
| Set | âœ… | âœ… (Multiset) | âœ… | âŒ | âŒ |
| Dict/Record | âœ… | âŒ | âŒ | âŒ | âŒ |
| Graph | âœ… | âœ… | âœ… | âœ… | âŒ |
| Tree | âŒ | âœ… | âŒ | âŒ | âŒ |
| FSA | âŒ | âœ… | âŒ | âŒ | âŒ |

**VSAX strength**: Clean, extensible encoder API with `AbstractEncoder` base class.

### Machine Learning

| Feature | VSAX | torchhd | hdlib | PyBHV | hdtorch |
|---------|------|---------|-------|-------|---------|
| Classification | âŒ | âœ… (9+ types) | âœ… | âœ… | âœ… (Basic) |
| Built-in Datasets | âŒ | âœ… (150+) | âœ… (Some) | âŒ | âŒ |
| Online Learning | âŒ | âœ… (OnlineHD) | âŒ | âŒ | âŒ |
| Neural Integration | âŒ | âœ… (NeuralHD) | âŒ | âŒ | âŒ |
| Regression | âŒ | âŒ | âœ… | âŒ | âŒ |
| Clustering | âŒ | âŒ | âœ… | âŒ | âŒ |

**Biggest VSAX gap**: No built-in classifiers or ML workflows (yet).

**torchhd is the clear winner** for production ML applications.

### Performance & Hardware

| Feature | VSAX | torchhd | hdlib | PyBHV | hdtorch |
|---------|------|---------|-------|-------|---------|
| GPU Support | âœ… (JAX auto) | âœ… (PyTorch) | âŒ | âœ… (PyTorch backend) | âœ… (Custom CUDA) |
| CPU Fallback | âœ… | âœ… | âœ… | âœ… | âŒ |
| Batch Operations | âœ… (vmap) | âœ… | âœ… | âœ… | âœ… |
| JIT Compilation | âœ… (JAX) | âœ… (TorchScript) | âŒ | âŒ | âœ… |
| Custom Kernels | âŒ | âŒ | âŒ | âœ… (C++) | âœ… (CUDA) |

**VSAX uses JAX's automatic GPU dispatch** - no manual device management.

### Developer Experience

| Feature | VSAX | torchhd | hdlib | PyBHV | hdtorch |
|---------|------|---------|-------|-------|---------|
| Type Hints | âœ… (Full) | âœ… | â“ | âŒ | â“ |
| Test Coverage | 94% | â“ | â“ | â“ | â“ |
| Documentation | âœ… | âœ… | âœ… (Wiki) | âœ… | âœ… |
| Tutorials | âœ… (3 deep) | âœ… | âœ… | âœ… (Examples) | âœ… |
| Examples | âœ… | âœ… | âœ… | âœ… (Many) | âœ… |

**VSAX prioritizes code quality**: Type-safe, well-tested, thoroughly documented.

---

## Detailed Library Comparison

### torchhd: The Production ML Library

**Best for**: Machine learning applications, classification tasks, production deployment

**Strengths**:
- **Comprehensive**: 8 VSA models, 9+ classifiers, 150+ datasets
- **Production-ready**: Battle-tested with active community (346 stars)
- **PyTorch integration**: Seamless with existing PyTorch workflows
- **Rich structures**: Graph, Tree, FSA, HashTable implementations
- **Well-documented**: Extensive tutorials and examples

**Weaknesses**:
- **Complexity**: Large API surface, steeper learning curve
- **PyTorch-coupled**: Hard to use without PyTorch knowledge
- **Less modular**: Models are monolithic classes

**When to choose over VSAX**:
- You need production ML classifiers
- You're already using PyTorch
- You want ready-made datasets
- You need advanced structures (Tree, FSA)

### hdlib: The Biomedical Specialist

**Best for**: Biomedical applications, bioinformatics, medical informatics

**Strengths**:
- **Domain focus**: Proven in cancer classification, metagenomics
- **Versatile**: Classification, regression, clustering, feature selection
- **Academic backing**: Peer-reviewed publications
- **Easy install**: PyPI and conda-forge

**Weaknesses**:
- **Less clear**: VSA model support not well documented
- **No GPU**: CPU-only implementation
- **Older codebase**: Less active maintenance

**When to choose over VSAX**:
- You're working in bioinformatics/medical AI
- You need regression or clustering
- You want proven biomedical applications

### PyBHV: The Boolean Specialist

**Best for**: Boolean operations, symbolic reasoning, theoretical research

**Strengths**:
- **Research framework**: Expression simplification, circuit compilation
- **Multiple backends**: Python, C++, NumPy, PyTorch with bit-packing
- **Rich metrics**: Comprehensive distance and similarity measures
- **Symbolic computing**: Law-based testing and optimization
- **Memory efficient**: 8x compression with bit-packing

**Weaknesses**:
- **Boolean only**: No support for real or complex hypervectors
- **Narrow focus**: Limited to binary VSA
- **Complex API**: Many abstraction levels

**When to choose over VSAX**:
- You only need boolean/binary hypervectors
- You want circuit compilation or logic synthesis
- You need bit-level optimization
- You're doing theoretical VSA research

### hdtorch: The CUDA Accelerator

**Best for**: Custom GPU kernels, maximum performance

**Strengths**:
- **Custom CUDA**: Hand-optimized GPU kernels
- **Performance**: Fastest for supported operations
- **Educational**: Clear tutorials on CUDA implementation

**Weaknesses**:
- **Limited scope**: Fewer features than torchhd or VSAX
- **CUDA required**: No CPU fallback
- **Less mature**: Smaller community

**When to choose over VSAX**:
- You need maximum GPU performance
- You want to learn CUDA kernel programming
- You're willing to trade features for speed

---

## What Makes VSAX Unique?

### 1. **JAX-First Design**

VSAX is the **only JAX-native VSA library**:

```python
# Automatic GPU acceleration
model = create_fhrr_model(dim=512)  # Works on GPU if available

# JIT compilation for speed
@jit
def encode_batch(items):
    return vmap(encoder.encode)(items)

# Automatic differentiation (future: differentiable VSA)
gradient = grad(lambda x: similarity(x, target))
```

**Why this matters**:
- JAX is the future of ML research (used by Google, DeepMind)
- Functional programming = easier reasoning
- Better for research and prototyping

### 2. **Clean Theoretical Foundation**

VSAX implements the **canonical VSA models** from foundational papers:

- **FHRR**: Plate (1995) - Complex-valued circular convolution
- **MAP**: Gayler (1998) - Multiply-Add-Permute
- **Binary**: Kanerva (1996) - Binary Spatter Codes

Each implementation is **mathematically correct** and **well-documented**.

### 3. **Resonator Networks**

VSAX is the **only library with full resonator support**:

```python
# Factorize compositional structures
resonator = Resonator(model, codebooks=[subjects, relations, objects])
factors = resonator.factorize(composite_vector)
# ['dog', 'isA', 'mammal']
```

Based on Frady et al. (2020), resonators enable:
- Decoding compositional structures
- Iterative refinement with convergence
- Multi-factor factorization

### 4. **Tutorial-Driven Documentation**

VSAX teaches **VSA concepts**, not just API:

1. **MNIST Classification**: Learn encoding and prototypes
2. **Knowledge Graphs**: Understand binding and bundling
3. **Kanerva's Analogies**: Master mappings and transformations

Each tutorial implements **foundational papers** with full code.

### 5. **Research-Friendly Architecture**

VSAX makes it **easy to experiment**:

```python
# Try different operations with same representation
model1 = VSAModel(dim=512, rep_cls=ComplexHypervector,
                  opset=FHRROperations(), sampler=sample_complex_random)

model2 = VSAModel(dim=512, rep_cls=ComplexHypervector,
                  opset=MAPOperations(), sampler=sample_complex_random)

# Same API, different algebra!
```

---

## What VSAX Doesn't (Yet) Do

We're honest about gaps:

### âŒ Machine Learning Classifiers

**Missing**:
- No built-in classifiers (Centroid, AdaptHD, OnlineHD, etc.)
- No datasets
- No training loops

**Workaround**: Build your own with VSAX primitives:
```python
# Manual centroid classifier
prototypes = {label: bundle(class_examples) for label, class_examples in data}
prediction = max(prototypes, key=lambda l: similarity(query, prototypes[l]))
```

**Future**: v1.0+ will add classifiers

### âŒ Advanced Structures

**Missing**:
- Tree encoders
- Finite State Automata
- HashTable structures

**Workaround**: Use GraphEncoder as building block

**Future**: May add in v1.x based on demand

### âŒ Additional VSA Models

**Missing**:
- HRR (original Plate model without FFT)
- BSC variants (Sparse Block Codes)
- CGR, MCR, VTB from recent research

**Reason**: We prioritize **depth** (correct implementation, documentation, tests) over **breadth**

**Future**: May add models with strong theoretical foundation

### âŒ Production Optimization

**Missing**:
- Custom CUDA kernels (like hdtorch)
- Bit-packing (like PyBHV)
- Quantization/compression

**Reason**: JAX provides good-enough performance for research

**Future**: Optimization in later versions if needed

---

## Choosing the Right Library

### Use VSAX if you:
- ğŸ“ Want to **learn VSA deeply** with tutorial-driven examples
- ğŸ”¬ Are doing **research** and need flexibility
- ğŸ§® Prefer **functional programming** and JAX
- ğŸ“ Value **theoretical correctness** over feature count
- ğŸ§© Need **compositional operations** (resonators, mappings)
- ğŸ’» Want **type-safe, well-tested** code

### Use torchhd if you:
- ğŸ­ Need **production ML** with classifiers and datasets
- ğŸ”¥ Are already using **PyTorch**
- ğŸ“Š Want **many VSA models** to experiment with
- ğŸš€ Need **battle-tested** software (350+ stars)
- ğŸ¯ Are building **classification systems**

### Use hdlib if you:
- ğŸ§¬ Work in **bioinformatics** or **medical AI**
- ğŸ“ˆ Need **regression** or **clustering**
- ğŸ“š Want **proven biomedical applications**
- ğŸ Prefer simple Python without GPU

### Use PyBHV if you:
- ğŸ”² Only need **boolean hypervectors**
- âš¡ Want **bit-level optimization**
- ğŸ§  Are doing **symbolic reasoning** research
- ğŸ”§ Need **circuit compilation**

### Use hdtorch if you:
- âš™ï¸ Need **custom CUDA kernels**
- ğŸï¸ Want **maximum GPU performance**
- ğŸ“ Want to **learn CUDA** programming

---

## VSAX Roadmap: Closing the Gaps

### v1.0.0 (Future)
- âœ… Basic classifiers (Centroid, kNN)
- âœ… Common datasets (MNIST, CIFAR-10)
- âœ… Training utilities

### v1.1.0 (Future)
- âœ… Tree and FSA encoders
- âœ… Additional VSA models (HRR, BSC variants)

### v2.0.0 (Future)
- âœ… Advanced classifiers (OnlineHD, AdaptHD)
- âœ… Performance optimizations
- âœ… Production tooling

**Guiding principle**: Maintain simplicity and theoretical clarity while adding practical features.

---

## Contributing to VSAX

We welcome contributions! Priority areas:

1. **Classifiers**: Implement standard HDC classifiers
2. **Datasets**: Add benchmark datasets with encoders
3. **Examples**: More domain applications (NLP, robotics, etc.)
4. **VSA Models**: Add models with theoretical grounding
5. **Performance**: Optimize hot paths while keeping API clean

See [CONTRIBUTING.md](https://github.com/vasanthsarathy/vsax/blob/main/CONTRIBUTING.md) for guidelines.

---

## Conclusion

**VSAX is a research-oriented, JAX-native VSA library** that prioritizes:
- âœ¨ **Clarity** over completeness
- ğŸ§® **Theory** over features
- ğŸ”¬ **Research** over production

If you need **production ML** â†’ choose **torchhd**
If you need **biomedical apps** â†’ choose **hdlib**
If you need **boolean operations** â†’ choose **PyBHV**
If you need **custom CUDA** â†’ choose **hdtorch**

**If you want to understand VSA deeply and build novel approaches** â†’ choose **VSAX** âœ¨

---

## References

- **torchhd**: https://github.com/hyperdimensional-computing/torchhd
- **hdlib**: https://github.com/cumbof/hdlib
- **PyBHV**: https://github.com/Adam-Vandervorst/PyBHV
- **hdtorch**: https://hdtorch.readthedocs.io/en/latest/

---

*Last updated: 2025-01-16*
